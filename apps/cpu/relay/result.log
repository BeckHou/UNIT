Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 16, 16), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:48] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 16, 16), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 16, 16), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 16, 16), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 16, 16, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9852 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 17.4799 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8547 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2923 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1312 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 179.058 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.9479 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.3509 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 17.6343 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.7294 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3682 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.16 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 184.692 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.1283 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0993 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.8086 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.271 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.373 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1443 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 195.07 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.7525 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.8681 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.3083 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.032 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.283 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1392 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 191.784 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.0218 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0996 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.4618 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.023 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3785 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1794 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 187.669 us/iter
[00:51:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 17.2061 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 16, 16), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:49] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 16, 16), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 16, 16), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 16, 16), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 16, 16, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.03 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.573 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8032 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3659 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1891 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 182.256 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.0918 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.021 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.4082 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8857 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3327 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.113 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 178.087 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.8505 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1518 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.7991 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8691 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3858 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1283 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 180.107 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.9136 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0735 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.9086 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.1586 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4312 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.224 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 180.75 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.0228 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9938 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.5927 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8377 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 5.5633 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3063 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 179.793 us/iter
[00:51:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.0184 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 16, 16), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:50] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 16, 16), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 16, 16), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 16, 16), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 16, 16, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9419 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.4476 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8792 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 4.9292 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2867 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 179.527 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.944 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.2325 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.3616 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8916 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2811 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1118 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 178.632 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.7787 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1438 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.6377 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.1684 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2843 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.211 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 199.275 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 17.3639 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9922 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.5303 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.0071 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.195 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0578 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 192.356 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.4328 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0553 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.4267 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.1545 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.295 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1283 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 202.046 us/iter
[00:51:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 17.2482 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 16, 16), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:52] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 16, 16), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 16, 16), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 16, 16), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 16, 16, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.8583 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.4984 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8018 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3305 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2647 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 184.066 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.2299 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1245 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.4716 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.9227 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.383 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2087 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 179.744 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.912 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1801 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.9387 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.0216 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4062 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2515 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 207.463 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.9978 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.2155 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.9039 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.5318 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4072 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2686 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 204.09 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 17.9898 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.8251 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.6232 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.9263 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3995 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2404 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 184.194 us/iter
[00:51:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.2625 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 16, 16), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:53] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 16, 16), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 16, 16), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 16, 16), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 16, 16, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0071 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.957 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.9733 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4407 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3191 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 177.596 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.0083 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.262 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.5232 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8514 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4458 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3098 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 180.767 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.0064 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1718 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.8649 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.2288 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.5171 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2354 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 198.652 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 18.1481 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.2869 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.9605 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.0082 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4303 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2532 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 193.627 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.2385 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.895 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.5894 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.1846 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4257 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3201 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 200.339 us/iter
[00:51:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 18.1572 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 16, 16), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:54] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 16, 16), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 16, 16), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 16, 16), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 16, 16, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1201 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 17.6232 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8984 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4456 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2112 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 187.071 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.1708 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.2929 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 17.7603 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.887 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4329 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 3.9306 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 180.043 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.0847 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.3631 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.2667 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.1441 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4547 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2282 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 205.337 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.785 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0037 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 17.7521 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8516 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4255 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2791 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 184.594 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.3281 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.2209 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 17.8158 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8453 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4007 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2092 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 182.254 us/iter
[00:51:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.1368 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 16, 16), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:55] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 16, 16), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 16, 16), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 16, 16), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 16, 16, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9804 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 17.37 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.7863 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2536 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1474 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 185.362 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.0365 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9101 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 17.3561 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8006 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.206 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.107 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 178.86 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.9673 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.4859 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 21.4394 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.0026 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2822 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1102 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 203.017 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.6062 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.8241 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.6563 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.4222 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2744 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1119 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 195.893 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.5506 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 12.0089 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.2139 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.9939 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.253 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1081 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 183.171 us/iter
[00:51:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.8635 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 16, 16), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:56] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 16, 16), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 16, 16), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 16, 16), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 16, 16, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.2465 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.9219 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.2756 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4154 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1716 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 178.561 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.8421 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.237 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.4905 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8364 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2709 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1828 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 184.541 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.0121 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.2199 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.9701 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.2777 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3002 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1663 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 205.909 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 17.1769 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.3117 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.108 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.1439 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2974 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1714 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 203.315 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.8578 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.994 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.8743 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.5989 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3274 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1315 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 200.135 us/iter
[00:51:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 17.5557 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 16, 16), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:57] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 16, 16), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 16, 16), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 16, 16), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 16, 16, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.2614 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 17.6744 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.7643 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3086 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1508 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 177.446 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.8799 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.7039 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 17.6625 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8773 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2445 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1324 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 178.064 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.8644 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.2954 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.4915 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.9915 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2606 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1928 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 179.937 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.8601 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.4663 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.7884 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.2013 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3258 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1819 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 200.805 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 17.2139 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 13.1937 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.5811 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.0915 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3588 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1761 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 185.583 us/iter
[00:51:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 16.2284 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 16, 16), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:58] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 16, 16), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 16, 16), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 16, 16, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 16, 16), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 16, 16, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0168 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.5038 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8875 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3612 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1898 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 184.53 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.9509 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1803 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.5112 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.9398 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3124 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1962 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 177.947 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 15.9506 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.3457 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.8162 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.1767 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3094 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2324 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 202.149 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 17.2244 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1005 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.6355 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.1609 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3377 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1698 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 200.57 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 17.0676 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0713 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.1254 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.2732 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2839 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3297 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 197.458 us/iter
[00:51:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 17.1516 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 960, 8, 8), 'uint8'), ('TENSOR', (160, 960, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:59] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 960, 8, 8), int8], %w: Tensor[(160, 960, 1, 1), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 960, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 60, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(960, 1, 1, 160), int8] */;
  %5 = reshape(%4, newshape=[960, 1, 1, 10, 16]) /* ty=Tensor[(960, 1, 1, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 1, 1, 16, 960), int8] */;
  %7 = reshape(%6, newshape=[10, 1, 1, 16, 60, 16]) /* ty=Tensor[(10, 1, 1, 16, 60, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 1, 1, 16, 60, 4, 4]) /* ty=Tensor[(10, 1, 1, 16, 60, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 60, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 960, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 60, 8, 8, 16), 'uint8'), ('TENSOR', (10, 60, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.40902 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.54552 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.46826 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.03713 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.00398 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.332 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.55948 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.06256 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.42537 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.47972 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.04442 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.01023 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.968 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.933 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.07754 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.63192 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.50846 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.39262 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07831 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.6888 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.06708 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.07396 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.46367 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.52516 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0516 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.03922 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.5155 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.31662 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.08493 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.81296 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7095 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11702 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07965 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.4383 us/iter
[00:51:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.23857 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 960, 8, 8), 'uint8'), ('TENSOR', (160, 960, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:00] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 960, 8, 8), int8], %w: Tensor[(160, 960, 1, 1), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 960, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 60, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(960, 1, 1, 160), int8] */;
  %5 = reshape(%4, newshape=[960, 1, 1, 10, 16]) /* ty=Tensor[(960, 1, 1, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 1, 1, 16, 960), int8] */;
  %7 = reshape(%6, newshape=[10, 1, 1, 16, 60, 16]) /* ty=Tensor[(10, 1, 1, 16, 60, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 1, 1, 16, 60, 4, 4]) /* ty=Tensor[(10, 1, 1, 16, 60, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 60, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 960, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 60, 8, 8, 16), 'uint8'), ('TENSOR', (10, 60, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.04304 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.49559 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.51291 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.039 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.03924 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.5581 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.6618 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.06964 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.5064 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.46942 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0434 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04278 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.911 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.7316 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.07049 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.54187 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.49082 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.04742 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.01962 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.7835 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.4551 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.02916 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.43938 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.46982 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.02367 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04582 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.7609 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.7442 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.05982 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.76213 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8318 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.05109 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05749 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.9331 us/iter
[00:52:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.78116 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 960, 8, 8), 'uint8'), ('TENSOR', (160, 960, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:00] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 960, 8, 8), int8], %w: Tensor[(160, 960, 1, 1), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 960, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 60, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(960, 1, 1, 160), int8] */;
  %5 = reshape(%4, newshape=[960, 1, 1, 10, 16]) /* ty=Tensor[(960, 1, 1, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 1, 1, 16, 960), int8] */;
  %7 = reshape(%6, newshape=[10, 1, 1, 16, 60, 16]) /* ty=Tensor[(10, 1, 1, 16, 60, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 1, 1, 16, 60, 4, 4]) /* ty=Tensor[(10, 1, 1, 16, 60, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 60, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 960, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 60, 8, 8, 16), 'uint8'), ('TENSOR', (10, 60, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.21507 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.39351 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7636 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14791 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.06833 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.5378 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.59589 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.27062 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.80787 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 3.29567 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.18696 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.11209 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.5466 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.1632 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.07916 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.80819 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.76347 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12249 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.29009 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.6012 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.55879 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.08397 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.44259 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.84469 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.129 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.41566 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.7721 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.57234 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.20264 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.51796 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8196 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13658 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.11307 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.9917 us/iter
[00:52:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.81456 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 960, 8, 8), 'uint8'), ('TENSOR', (160, 960, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:01] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 960, 8, 8), int8], %w: Tensor[(160, 960, 1, 1), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 960, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 60, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(960, 1, 1, 160), int8] */;
  %5 = reshape(%4, newshape=[960, 1, 1, 10, 16]) /* ty=Tensor[(960, 1, 1, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 1, 1, 16, 960), int8] */;
  %7 = reshape(%6, newshape=[10, 1, 1, 16, 60, 16]) /* ty=Tensor[(10, 1, 1, 16, 60, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 1, 1, 16, 60, 4, 4]) /* ty=Tensor[(10, 1, 1, 16, 60, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 60, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 960, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 60, 8, 8, 16), 'uint8'), ('TENSOR', (10, 60, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.19642 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.36409 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.47916 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.03356 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 1.99247 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.2726 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.06709 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.9932 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.66986 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 3.12296 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.05427 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.01461 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.3822 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.80255 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.9958 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.43818 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.43092 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.07502 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 1.96976 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.5131 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.12775 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.01784 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.4871 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7079 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.35222 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.00304 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.675 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.252 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.9903 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.44034 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.94798 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.06544 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.15914 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.6993 us/iter
[00:52:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.58068 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 960, 8, 8), 'uint8'), ('TENSOR', (160, 960, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:02] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 960, 8, 8), int8], %w: Tensor[(160, 960, 1, 1), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 960, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 60, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(960, 1, 1, 160), int8] */;
  %5 = reshape(%4, newshape=[960, 1, 1, 10, 16]) /* ty=Tensor[(960, 1, 1, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 1, 1, 16, 960), int8] */;
  %7 = reshape(%6, newshape=[10, 1, 1, 16, 60, 16]) /* ty=Tensor[(10, 1, 1, 16, 60, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 1, 1, 16, 60, 4, 4]) /* ty=Tensor[(10, 1, 1, 16, 60, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 60, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 960, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 60, 8, 8, 16), 'uint8'), ('TENSOR', (10, 60, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.01688 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.31669 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.47441 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.02718 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.01773 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.3674 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.44273 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.0136 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.36587 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.5017 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.02243 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.00491 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.5024 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.80838 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.51228 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.383 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.512 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.28484 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.03008 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.5939 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.48934 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.059 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.38453 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.5278 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.05831 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0191 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.8193 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.10751 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.0448 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.37158 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.51886 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.06146 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0437 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.6386 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.86336 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 960, 8, 8), 'uint8'), ('TENSOR', (160, 960, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:03] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 960, 8, 8), int8], %w: Tensor[(160, 960, 1, 1), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 960, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 60, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(960, 1, 1, 160), int8] */;
  %5 = reshape(%4, newshape=[960, 1, 1, 10, 16]) /* ty=Tensor[(960, 1, 1, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 1, 1, 16, 960), int8] */;
  %7 = reshape(%6, newshape=[10, 1, 1, 16, 60, 16]) /* ty=Tensor[(10, 1, 1, 16, 60, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 1, 1, 16, 60, 4, 4]) /* ty=Tensor[(10, 1, 1, 16, 60, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 60, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 960, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 60, 8, 8, 16), 'uint8'), ('TENSOR', (10, 60, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.9973 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.83333 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.47752 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.06174 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05443 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.456 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.10546 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.03226 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.49063 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.46576 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.04017 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.01296 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.9142 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.48552 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.04196 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.42651 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.44671 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08076 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.38924 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.911 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.4944 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.987 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.43807 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.41988 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.06995 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.01995 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.485 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.55495 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.01804 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.46274 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.43513 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08715 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.01274 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.5551 us/iter
[00:52:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.30276 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 960, 8, 8), 'uint8'), ('TENSOR', (160, 960, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:04] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 960, 8, 8), int8], %w: Tensor[(160, 960, 1, 1), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 960, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 60, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(960, 1, 1, 160), int8] */;
  %5 = reshape(%4, newshape=[960, 1, 1, 10, 16]) /* ty=Tensor[(960, 1, 1, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 1, 1, 16, 960), int8] */;
  %7 = reshape(%6, newshape=[10, 1, 1, 16, 60, 16]) /* ty=Tensor[(10, 1, 1, 16, 60, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 1, 1, 16, 60, 4, 4]) /* ty=Tensor[(10, 1, 1, 16, 60, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 60, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 960, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 60, 8, 8, 16), 'uint8'), ('TENSOR', (10, 60, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.02954 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.37664 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.48434 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.05232 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05218 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.3704 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.18754 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.99658 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7635 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.49104 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09698 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05646 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.4212 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.33206 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.06983 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.44473 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.87431 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.22694 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.06542 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.69 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.48069 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.05459 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.48194 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.51055 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0831 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08722 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.8246 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.88251 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.05767 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.42685 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.5044 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08465 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07013 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.4999 us/iter
[00:52:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.45852 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 960, 8, 8), 'uint8'), ('TENSOR', (160, 960, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:05] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 960, 8, 8), int8], %w: Tensor[(160, 960, 1, 1), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 960, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 60, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(960, 1, 1, 160), int8] */;
  %5 = reshape(%4, newshape=[960, 1, 1, 10, 16]) /* ty=Tensor[(960, 1, 1, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 1, 1, 16, 960), int8] */;
  %7 = reshape(%6, newshape=[10, 1, 1, 16, 60, 16]) /* ty=Tensor[(10, 1, 1, 16, 60, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 1, 1, 16, 60, 4, 4]) /* ty=Tensor[(10, 1, 1, 16, 60, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 60, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 960, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 60, 8, 8, 16), 'uint8'), ('TENSOR', (10, 60, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.98919 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.49114 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.70148 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.24883 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 1.98498 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.2675 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.09326 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.24073 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.66273 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.57211 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.25129 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.01384 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.9238 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.90818 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.23828 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 4.07442 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.54002 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.01407 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2216 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.6703 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.8563 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.22489 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.49427 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.96444 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12633 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.03916 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.9533 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.0857 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.69228 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 4.05179 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.572 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.07062 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05583 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 11.2124 us/iter
[00:52:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.0472 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 960, 8, 8), 'uint8'), ('TENSOR', (160, 960, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:06] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 960, 8, 8), int8], %w: Tensor[(160, 960, 1, 1), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 960, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 60, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(960, 1, 1, 160), int8] */;
  %5 = reshape(%4, newshape=[960, 1, 1, 10, 16]) /* ty=Tensor[(960, 1, 1, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 1, 1, 16, 960), int8] */;
  %7 = reshape(%6, newshape=[10, 1, 1, 16, 60, 16]) /* ty=Tensor[(10, 1, 1, 16, 60, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 1, 1, 16, 60, 4, 4]) /* ty=Tensor[(10, 1, 1, 16, 60, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 60, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 960, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 60, 8, 8, 16), 'uint8'), ('TENSOR', (10, 60, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.10822 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.58446 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.57448 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11824 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.351 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.8258 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.2023 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.1502 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.54757 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.56331 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11118 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04682 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.7229 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.5188 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.12004 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.56052 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.59579 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12962 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.10788 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.8227 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.9271 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.07698 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.55167 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.56679 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09719 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04312 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.8456 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.63421 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.12804 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.56084 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.54536 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1298 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0728 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 11.2188 us/iter
[00:52:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.27776 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 960, 8, 8), 'uint8'), ('TENSOR', (160, 960, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:06] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 960, 8, 8), int8], %w: Tensor[(160, 960, 1, 1), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 960, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 960, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 60, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(960, 1, 1, 160), int8] */;
  %5 = reshape(%4, newshape=[960, 1, 1, 10, 16]) /* ty=Tensor[(960, 1, 1, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 1, 1, 16, 960), int8] */;
  %7 = reshape(%6, newshape=[10, 1, 1, 16, 60, 16]) /* ty=Tensor[(10, 1, 1, 16, 60, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 1, 1, 16, 60, 4, 4]) /* ty=Tensor[(10, 1, 1, 16, 60, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 60, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 960, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 60, 8, 8, 16), 'uint8'), ('TENSOR', (10, 60, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.007 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.43575 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.43065 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.01363 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 1.97604 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.2365 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.33225 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.05937 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.47898 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.48039 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.02145 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.26771 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.4072 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.05704 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.00075 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.43259 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.44249 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 1.98222 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.15596 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.6164 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.18984 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.97878 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.666 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.41602 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.02796 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0158 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.5881 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.72508 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.96833 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.42443 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.44253 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.02778 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.15449 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 10.5095 us/iter
[00:52:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.54851 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 256, 56, 56), 'uint8'), ('TENSOR', (512, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:07] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 256, 56, 56), int8], %w: Tensor[(512, 256, 1, 1), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 256, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(256, 1, 1, 512), int8] */;
  %5 = reshape(%4, newshape=[256, 1, 1, 32, 16]) /* ty=Tensor[(256, 1, 1, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 1, 1, 16, 256), int8] */;
  %7 = reshape(%6, newshape=[32, 1, 1, 16, 16, 16]) /* ty=Tensor[(32, 1, 1, 16, 16, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 1, 1, 16, 16, 4, 4]) /* ty=Tensor[(32, 1, 1, 16, 16, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 16, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 256, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 16, 56, 56, 16), 'uint8'), ('TENSOR', (32, 16, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.2664 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.6641 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6017 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3622 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.251 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 109.52 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 95.2825 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.7657 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7311 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.585 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3217 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2886 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 110.941 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 92.086 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.6191 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7237 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.691 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4437 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3689 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 117.835 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 96.3534 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 21.0381 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8195 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6941 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3514 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.392 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 115.331 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 99.8064 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.5302 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.6268 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.5945 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3403 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2486 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 121.94 us/iter
[00:52:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 96.535 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 256, 56, 56), 'uint8'), ('TENSOR', (512, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:09] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 256, 56, 56), int8], %w: Tensor[(512, 256, 1, 1), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 256, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(256, 1, 1, 512), int8] */;
  %5 = reshape(%4, newshape=[256, 1, 1, 32, 16]) /* ty=Tensor[(256, 1, 1, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 1, 1, 16, 256), int8] */;
  %7 = reshape(%6, newshape=[32, 1, 1, 16, 16, 16]) /* ty=Tensor[(32, 1, 1, 16, 16, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 1, 1, 16, 16, 4, 4]) /* ty=Tensor[(32, 1, 1, 16, 16, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 16, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 256, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 16, 56, 56, 16), 'uint8'), ('TENSOR', (32, 16, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.66 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.6615 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6614 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2995 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2572 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 119.873 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 93.0851 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.8606 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7788 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6978 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3635 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.353 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 111.512 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 92.6251 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 20.2227 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7374 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6627 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.418 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2981 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 114.591 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 99.0199 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.8957 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8041 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6752 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3322 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3363 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 114.034 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 94.7527 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 20.0525 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.844 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6751 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3632 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3255 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 119.634 us/iter
[00:52:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 97.9989 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 256, 56, 56), 'uint8'), ('TENSOR', (512, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:10] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 256, 56, 56), int8], %w: Tensor[(512, 256, 1, 1), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 256, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(256, 1, 1, 512), int8] */;
  %5 = reshape(%4, newshape=[256, 1, 1, 32, 16]) /* ty=Tensor[(256, 1, 1, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 1, 1, 16, 256), int8] */;
  %7 = reshape(%6, newshape=[32, 1, 1, 16, 16, 16]) /* ty=Tensor[(32, 1, 1, 16, 16, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 1, 1, 16, 16, 4, 4]) /* ty=Tensor[(32, 1, 1, 16, 16, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 16, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 256, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 16, 56, 56, 16), 'uint8'), ('TENSOR', (32, 16, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.8419 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8488 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7842 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4519 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4771 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 114.223 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 95.7288 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.4477 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7861 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6663 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3435 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3713 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 111.57 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 92.3451 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.4836 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7636 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6891 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3734 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3473 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 119.007 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 96.5497 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.9165 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8746 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6747 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4723 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4135 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 115.624 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 100.803 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.5325 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7475 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7823 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3851 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.358 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 123.147 us/iter
[00:52:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 96.8946 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 256, 56, 56), 'uint8'), ('TENSOR', (512, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:11] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 256, 56, 56), int8], %w: Tensor[(512, 256, 1, 1), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 256, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(256, 1, 1, 512), int8] */;
  %5 = reshape(%4, newshape=[256, 1, 1, 32, 16]) /* ty=Tensor[(256, 1, 1, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 1, 1, 16, 256), int8] */;
  %7 = reshape(%6, newshape=[32, 1, 1, 16, 16, 16]) /* ty=Tensor[(32, 1, 1, 16, 16, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 1, 1, 16, 16, 4, 4]) /* ty=Tensor[(32, 1, 1, 16, 16, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 16, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 256, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 16, 56, 56, 16), 'uint8'), ('TENSOR', (32, 16, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 18.9399 us/iter
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7239 us/iter
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.5852 us/iter
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3212 us/iter
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2718 us/iter
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 112.962 us/iter
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 91.5365 us/iter
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.6372 us/iter
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8981 us/iter
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7513 us/iter
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4075 us/iter
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2554 us/iter
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 110.617 us/iter
[00:52:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 92.5907 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.3797 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8223 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6605 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3975 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3625 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 116.155 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 99.5735 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 22.5026 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 4.0571 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6688 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3985 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2775 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 116.455 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 93.7163 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.446 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.883 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6661 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4888 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2888 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 117.951 us/iter
[00:52:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 97.5469 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 256, 56, 56), 'uint8'), ('TENSOR', (512, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:12] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 256, 56, 56), int8], %w: Tensor[(512, 256, 1, 1), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 256, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(256, 1, 1, 512), int8] */;
  %5 = reshape(%4, newshape=[256, 1, 1, 32, 16]) /* ty=Tensor[(256, 1, 1, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 1, 1, 16, 256), int8] */;
  %7 = reshape(%6, newshape=[32, 1, 1, 16, 16, 16]) /* ty=Tensor[(32, 1, 1, 16, 16, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 1, 1, 16, 16, 4, 4]) /* ty=Tensor[(32, 1, 1, 16, 16, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 16, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 256, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 16, 56, 56, 16), 'uint8'), ('TENSOR', (32, 16, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.5807 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7304 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6508 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.271 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2352 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 110.516 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 93.6246 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.9207 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7269 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6788 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2366 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2016 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 111.15 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 93.7191 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.6846 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7642 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7029 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3158 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2775 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 120.59 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 97.9905 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.6357 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7049 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6344 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2814 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1976 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 124.266 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 98.3503 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.5233 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7246 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6299 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2888 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1899 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 118.954 us/iter
[00:52:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 97.0699 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 256, 56, 56), 'uint8'), ('TENSOR', (512, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:13] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 256, 56, 56), int8], %w: Tensor[(512, 256, 1, 1), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 256, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(256, 1, 1, 512), int8] */;
  %5 = reshape(%4, newshape=[256, 1, 1, 32, 16]) /* ty=Tensor[(256, 1, 1, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 1, 1, 16, 256), int8] */;
  %7 = reshape(%6, newshape=[32, 1, 1, 16, 16, 16]) /* ty=Tensor[(32, 1, 1, 16, 16, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 1, 1, 16, 16, 4, 4]) /* ty=Tensor[(32, 1, 1, 16, 16, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 16, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 256, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 16, 56, 56, 16), 'uint8'), ('TENSOR', (32, 16, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.9249 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.6864 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6185 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2919 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2623 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 110.68 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 94.1088 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 20.2816 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7306 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.638 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3346 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2405 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 111.512 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 95.9098 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 20.3247 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 6.924 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7113 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3419 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3138 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 114.652 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 95.89 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.6361 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.802 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6432 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3083 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2584 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 119.317 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 94.8598 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 20.3093 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7283 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6837 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3006 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3026 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 119.201 us/iter
[00:52:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 98.918 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 256, 56, 56), 'uint8'), ('TENSOR', (512, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:15] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 256, 56, 56), int8], %w: Tensor[(512, 256, 1, 1), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 256, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(256, 1, 1, 512), int8] */;
  %5 = reshape(%4, newshape=[256, 1, 1, 32, 16]) /* ty=Tensor[(256, 1, 1, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 1, 1, 16, 256), int8] */;
  %7 = reshape(%6, newshape=[32, 1, 1, 16, 16, 16]) /* ty=Tensor[(32, 1, 1, 16, 16, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 1, 1, 16, 16, 4, 4]) /* ty=Tensor[(32, 1, 1, 16, 16, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 16, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 256, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 16, 56, 56, 16), 'uint8'), ('TENSOR', (32, 16, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.0559 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7797 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6157 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2473 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2587 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 113.717 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 93.2729 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.0794 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7968 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.5939 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2622 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3117 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 111.538 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 92.5603 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.3987 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.6995 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6186 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2995 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3036 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 119.516 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 94.9058 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 18.9537 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7575 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.5301 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3132 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2872 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 118.205 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 96.1686 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.2447 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.9681 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6185 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2753 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2556 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 121.66 us/iter
[00:52:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 96.2827 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 256, 56, 56), 'uint8'), ('TENSOR', (512, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:16] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 256, 56, 56), int8], %w: Tensor[(512, 256, 1, 1), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 256, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(256, 1, 1, 512), int8] */;
  %5 = reshape(%4, newshape=[256, 1, 1, 32, 16]) /* ty=Tensor[(256, 1, 1, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 1, 1, 16, 256), int8] */;
  %7 = reshape(%6, newshape=[32, 1, 1, 16, 16, 16]) /* ty=Tensor[(32, 1, 1, 16, 16, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 1, 1, 16, 16, 4, 4]) /* ty=Tensor[(32, 1, 1, 16, 16, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 16, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 256, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 16, 56, 56, 16), 'uint8'), ('TENSOR', (32, 16, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.5465 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 6.4205 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7808 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3302 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4115 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 112.705 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 91.9832 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.5302 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8278 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.5848 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4155 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3346 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 113.869 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 91.031 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.9236 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 5.5815 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8502 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4108 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3506 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 116.415 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 90.8997 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.6341 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.786 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6144 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3163 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2824 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 114.03 us/iter
[00:52:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 92.293 us/iter
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.5043 us/iter
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7326 us/iter
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.51 us/iter
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.6314 us/iter
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.39 us/iter
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 116.9 us/iter
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 97.5984 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 256, 56, 56), 'uint8'), ('TENSOR', (512, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:17] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 256, 56, 56), int8], %w: Tensor[(512, 256, 1, 1), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 256, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(256, 1, 1, 512), int8] */;
  %5 = reshape(%4, newshape=[256, 1, 1, 32, 16]) /* ty=Tensor[(256, 1, 1, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 1, 1, 16, 256), int8] */;
  %7 = reshape(%6, newshape=[32, 1, 1, 16, 16, 16]) /* ty=Tensor[(32, 1, 1, 16, 16, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 1, 1, 16, 16, 4, 4]) /* ty=Tensor[(32, 1, 1, 16, 16, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 16, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 256, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 16, 56, 56, 16), 'uint8'), ('TENSOR', (32, 16, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.3882 us/iter
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7412 us/iter
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6472 us/iter
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3603 us/iter
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2875 us/iter
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 113.188 us/iter
[00:52:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 92.7471 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 21.2677 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 4.1981 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6712 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3668 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3814 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 111.729 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 92.8552 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.491 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8568 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7008 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3556 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3391 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 116.851 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 101.563 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 20.0799 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8419 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6717 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3787 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3299 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 121.254 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 97.341 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 20.5188 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7313 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6987 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4304 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3615 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 113.767 us/iter
[00:52:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 95.5393 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 256, 56, 56), 'uint8'), ('TENSOR', (512, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:18] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 256, 56, 56), int8], %w: Tensor[(512, 256, 1, 1), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 256, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 256, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(256, 1, 1, 512), int8] */;
  %5 = reshape(%4, newshape=[256, 1, 1, 32, 16]) /* ty=Tensor[(256, 1, 1, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 1, 1, 16, 256), int8] */;
  %7 = reshape(%6, newshape=[32, 1, 1, 16, 16, 16]) /* ty=Tensor[(32, 1, 1, 16, 16, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 1, 1, 16, 16, 4, 4]) /* ty=Tensor[(32, 1, 1, 16, 16, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 16, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 256, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 16, 56, 56, 16), 'uint8'), ('TENSOR', (32, 16, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.4394 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.6944 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6724 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3451 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3282 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 114.423 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 92.105 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.3379 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7491 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7482 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3746 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3179 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 111.916 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 91.838 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.5824 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 5.1366 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8235 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3867 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.355 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 113.268 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 93.7838 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.2041 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.809 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7136 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3673 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3062 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 115.878 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 92.6574 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 19.6229 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7727 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7189 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3693 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.287 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 117.081 us/iter
[00:52:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 97.4183 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 56, 56), 'uint8'), ('TENSOR', (128, 64, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:20] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 56, 56), int8], %w: Tensor[(128, 64, 1, 1), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 1, 1, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 1, 1, 8, 16]) /* ty=Tensor[(64, 1, 1, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 1, 1, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 1, 1, 16, 4, 16]) /* ty=Tensor[(8, 1, 1, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 1, 1, 16, 4, 4, 4]) /* ty=Tensor[(8, 1, 1, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 56, 56, 16), 'uint8'), ('TENSOR', (8, 4, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 7.79506 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.52844 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.46375 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.16069 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.14056 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.8771 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.7264 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.819 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 4.08081 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.42144 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.19813 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.10713 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.6823 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.6005 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.7715 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.46481 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.41288 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11487 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09331 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 48.5348 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.4379 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.78131 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.4825 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.37062 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14356 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1255 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.7472 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.7874 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.9648 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.5572 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4704 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1933 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.198 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 48.468 us/iter
[00:52:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 33.0628 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 56, 56), 'uint8'), ('TENSOR', (128, 64, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:20] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 56, 56), int8], %w: Tensor[(128, 64, 1, 1), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 1, 1, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 1, 1, 8, 16]) /* ty=Tensor[(64, 1, 1, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 1, 1, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 1, 1, 16, 4, 16]) /* ty=Tensor[(8, 1, 1, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 1, 1, 16, 4, 4, 4]) /* ty=Tensor[(8, 1, 1, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 56, 56, 16), 'uint8'), ('TENSOR', (8, 4, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.77875 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.4185 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.31438 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11194 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.065 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 35.0033 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.7263 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.93744 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.48575 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.32506 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.07881 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08462 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 34.0274 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.7779 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.79519 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.50813 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.29937 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08375 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.06313 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 38.9453 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.398 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.84463 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.50681 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.32462 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08581 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.03019 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 38.6362 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.4256 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.86744 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.46106 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.32213 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08681 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04694 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 36.4242 us/iter
[00:52:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.8846 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 56, 56), 'uint8'), ('TENSOR', (128, 64, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:21] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 56, 56), int8], %w: Tensor[(128, 64, 1, 1), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 1, 1, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 1, 1, 8, 16]) /* ty=Tensor[(64, 1, 1, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 1, 1, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 1, 1, 16, 4, 16]) /* ty=Tensor[(8, 1, 1, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 1, 1, 16, 4, 4, 4]) /* ty=Tensor[(8, 1, 1, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 56, 56, 16), 'uint8'), ('TENSOR', (8, 4, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.80869 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.51438 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3625 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.06825 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.01413 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.4606 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.2104 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.88944 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.49369 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.36269 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.94875 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.12437 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 44.1585 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.2172 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.86856 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.5125 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.32625 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12356 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05562 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 39.2147 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.9719 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.82981 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.49513 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.39131 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 3.13313 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.13831 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 37.6965 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.8224 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.93462 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.46494 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.36512 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10456 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07675 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 39.439 us/iter
[00:52:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.7548 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 56, 56), 'uint8'), ('TENSOR', (128, 64, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:22] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 56, 56), int8], %w: Tensor[(128, 64, 1, 1), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 1, 1, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 1, 1, 8, 16]) /* ty=Tensor[(64, 1, 1, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 1, 1, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 1, 1, 16, 4, 16]) /* ty=Tensor[(8, 1, 1, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 1, 1, 16, 4, 4, 4]) /* ty=Tensor[(8, 1, 1, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 56, 56, 16), 'uint8'), ('TENSOR', (8, 4, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.9519 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.6739 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4253 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.135 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0993 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 56.4241 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.5474 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.72969 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.59613 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.46819 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13025 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08763 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 56.8067 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.4791 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 7.0309 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.5482 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.373 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0975 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1189 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 52.0832 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 34.394 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.9574 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.4971 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4424 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1775 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.087 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 56.1923 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.4392 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 7.017 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.4571 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4324 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1432 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0949 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 55.835 us/iter
[00:52:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.9 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 56, 56), 'uint8'), ('TENSOR', (128, 64, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:23] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 56, 56), int8], %w: Tensor[(128, 64, 1, 1), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 1, 1, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 1, 1, 8, 16]) /* ty=Tensor[(64, 1, 1, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 1, 1, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 1, 1, 16, 4, 16]) /* ty=Tensor[(8, 1, 1, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 1, 1, 16, 4, 4, 4]) /* ty=Tensor[(8, 1, 1, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 56, 56, 16), 'uint8'), ('TENSOR', (8, 4, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.7155 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.41075 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.33444 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.07788 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05494 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 30.4291 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.2939 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.73625 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.40431 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.32569 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12906 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04206 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 34.2066 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.3438 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.863 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.42644 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.32688 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10819 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07238 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 38.4149 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.0485 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.89119 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.44919 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.38125 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11731 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05244 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 38.3933 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.1542 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.83075 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.46137 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.33725 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.06725 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.06356 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 39.6685 us/iter
[00:52:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.2407 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 56, 56), 'uint8'), ('TENSOR', (128, 64, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:24] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 56, 56), int8], %w: Tensor[(128, 64, 1, 1), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 1, 1, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 1, 1, 8, 16]) /* ty=Tensor[(64, 1, 1, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 1, 1, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 1, 1, 16, 4, 16]) /* ty=Tensor[(8, 1, 1, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 1, 1, 16, 4, 4, 4]) /* ty=Tensor[(8, 1, 1, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 56, 56, 16), 'uint8'), ('TENSOR', (8, 4, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.80956 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.46319 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.38856 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.07919 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.03744 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 49.3249 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.8569 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.79906 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.46575 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.38444 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.04387 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.02987 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 47.9398 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.427 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.75006 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.42875 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.33606 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.04281 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08388 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 44.4482 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.4909 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.92088 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.43381 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.36987 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.71831 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04969 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 47.1119 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 33.0984 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.8825 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.49113 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.32081 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.06775 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05525 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.8814 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.5604 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 56, 56), 'uint8'), ('TENSOR', (128, 64, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:25] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 56, 56), int8], %w: Tensor[(128, 64, 1, 1), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 1, 1, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 1, 1, 8, 16]) /* ty=Tensor[(64, 1, 1, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 1, 1, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 1, 1, 16, 4, 16]) /* ty=Tensor[(8, 1, 1, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 1, 1, 16, 4, 4, 4]) /* ty=Tensor[(8, 1, 1, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 56, 56, 16), 'uint8'), ('TENSOR', (8, 4, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 7.0004 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.4687 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4092 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1385 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1017 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 56.1643 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.6941 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.8016 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.3828 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3728 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.088 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1219 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 52.7979 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.7749 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.7883 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.4002 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.39 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1394 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1289 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 56.4499 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.7104 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.7621 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.3864 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3493 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1204 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0889 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 56.2378 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 33.1878 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.9104 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.4577 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3755 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1528 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.067 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 56.2348 us/iter
[00:52:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 33.6561 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 56, 56), 'uint8'), ('TENSOR', (128, 64, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:26] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 56, 56), int8], %w: Tensor[(128, 64, 1, 1), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 1, 1, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 1, 1, 8, 16]) /* ty=Tensor[(64, 1, 1, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 1, 1, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 1, 1, 16, 4, 16]) /* ty=Tensor[(8, 1, 1, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 1, 1, 16, 4, 4, 4]) /* ty=Tensor[(8, 1, 1, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 56, 56, 16), 'uint8'), ('TENSOR', (8, 4, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 7.00287 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.67494 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.5525 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.24644 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.15506 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 33.2626 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.654 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.86612 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.61138 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.57569 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.22175 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.13675 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 35.5303 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.8603 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.96269 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.65394 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.54806 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.25419 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.18219 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 34.7173 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.8652 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.93606 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.65469 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.54763 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.21687 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.14494 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 36.7576 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.8547 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 7.04775 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.59781 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.54013 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.21119 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.13881 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 35.761 us/iter
[00:52:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.9999 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 56, 56), 'uint8'), ('TENSOR', (128, 64, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:27] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 56, 56), int8], %w: Tensor[(128, 64, 1, 1), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 1, 1, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 1, 1, 8, 16]) /* ty=Tensor[(64, 1, 1, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 1, 1, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 1, 1, 16, 4, 16]) /* ty=Tensor[(8, 1, 1, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 1, 1, 16, 4, 4, 4]) /* ty=Tensor[(8, 1, 1, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 56, 56, 16), 'uint8'), ('TENSOR', (8, 4, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.73812 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.45644 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 3.20056 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.64581 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.02163 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 30.5066 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.2158 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.75875 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.4385 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.34106 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.00387 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 1.96737 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 33.3681 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.1813 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.93131 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.43294 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.34463 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.04944 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.02781 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 37.7323 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.2076 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.86756 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.41519 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.97213 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.06469 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 1.99306 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 32.8444 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.6951 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.93181 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.47969 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.36881 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.07706 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.03444 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 31.0608 us/iter
[00:52:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 32.4213 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 56, 56), 'uint8'), ('TENSOR', (128, 64, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:28] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 56, 56), int8], %w: Tensor[(128, 64, 1, 1), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 56, 56), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 56, 56), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 56, 56), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 56, 56, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 1, 1, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 1, 1, 8, 16]) /* ty=Tensor[(64, 1, 1, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 1, 1, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 1, 1, 16, 4, 16]) /* ty=Tensor[(8, 1, 1, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 1, 1, 16, 4, 4, 4]) /* ty=Tensor[(8, 1, 1, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 56, 56, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 56, 56), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 56, 56, 16), 'uint8'), ('TENSOR', (8, 4, 1, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.92725 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.566 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.43381 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13969 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.15356 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 30.9272 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.1771 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.9465 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.54312 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.49494 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.138 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.14725 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 29.1659 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.2911 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.87069 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.49794 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.428 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12219 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.17056 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 29.7301 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.3072 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.88994 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.56762 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.41487 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.16994 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.14606 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 33.9964 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.2913 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.8585 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 2.53562 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.42325 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12994 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.13938 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 33.7388 us/iter
[00:52:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 31.3492 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 512, 10, 10), 'uint8'), ('TENSOR', (512, 512, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:29] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 512, 10, 10), int8], %w: Tensor[(512, 512, 3, 3), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 512, 10, 10), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 10, 10, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(512, 3, 3, 512), int8] */;
  %5 = reshape(%4, newshape=[512, 3, 3, 32, 16]) /* ty=Tensor[(512, 3, 3, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 3, 3, 16, 512), int8] */;
  %7 = reshape(%6, newshape=[32, 3, 3, 16, 32, 16]) /* ty=Tensor[(32, 3, 3, 16, 32, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 3, 3, 16, 32, 4, 4]) /* ty=Tensor[(32, 3, 3, 16, 32, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 32, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 512, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 32, 10, 10, 16), 'uint8'), ('TENSOR', (32, 32, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3906 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 47.6096 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.202 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2556 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1639 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.2153 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.0772 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.2401 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 48.9078 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 22.2987 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.363 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1565 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.9293 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.9579 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3849 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 48.7513 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.2901 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2679 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1555 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.4401 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.9438 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3648 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 49.4383 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.35 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2456 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1599 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.831 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.7846 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3687 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 49.0053 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.2295 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2494 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.166 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.8725 us/iter
[00:52:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.1863 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 512, 10, 10), 'uint8'), ('TENSOR', (512, 512, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:30] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 512, 10, 10), int8], %w: Tensor[(512, 512, 3, 3), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 512, 10, 10), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 10, 10, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(512, 3, 3, 512), int8] */;
  %5 = reshape(%4, newshape=[512, 3, 3, 32, 16]) /* ty=Tensor[(512, 3, 3, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 3, 3, 16, 512), int8] */;
  %7 = reshape(%6, newshape=[32, 3, 3, 16, 32, 16]) /* ty=Tensor[(32, 3, 3, 16, 32, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 3, 3, 16, 32, 4, 4]) /* ty=Tensor[(32, 3, 3, 16, 32, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 32, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 512, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 32, 10, 10, 16), 'uint8'), ('TENSOR', (32, 32, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 4.6657 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 48.875 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.3428 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3739 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3042 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.029 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.9557 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.4028 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 50.1764 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.4439 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.34 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2892 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.2933 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.6603 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 4.5219 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 50.6586 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.8898 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3513 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2834 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 44.9548 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.2443 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 4.6756 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 50.478 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.3457 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.362 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2259 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.6965 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.5257 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.4783 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 50.4378 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 21.0355 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3247 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 3.1512 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.018 us/iter
[00:52:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.8981 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 512, 10, 10), 'uint8'), ('TENSOR', (512, 512, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:31] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 512, 10, 10), int8], %w: Tensor[(512, 512, 3, 3), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 512, 10, 10), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 10, 10, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(512, 3, 3, 512), int8] */;
  %5 = reshape(%4, newshape=[512, 3, 3, 32, 16]) /* ty=Tensor[(512, 3, 3, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 3, 3, 16, 512), int8] */;
  %7 = reshape(%6, newshape=[32, 3, 3, 16, 32, 16]) /* ty=Tensor[(32, 3, 3, 16, 32, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 3, 3, 16, 32, 4, 4]) /* ty=Tensor[(32, 3, 3, 16, 32, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 32, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 512, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 32, 10, 10, 16), 'uint8'), ('TENSOR', (32, 32, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.7181 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 50.5712 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.277 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.134 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0802 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.7732 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.7744 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.2851 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 48.8243 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.2836 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1438 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1236 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.6003 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.6469 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.2067 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 48.9413 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.8526 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1648 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0835 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.7362 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.6908 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3404 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 51.5731 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.3703 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1161 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0863 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.8922 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.829 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.1834 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 50.0544 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 22.9021 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2427 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0854 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.2662 us/iter
[00:52:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.6837 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 512, 10, 10), 'uint8'), ('TENSOR', (512, 512, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:32] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 512, 10, 10), int8], %w: Tensor[(512, 512, 3, 3), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 512, 10, 10), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 10, 10, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(512, 3, 3, 512), int8] */;
  %5 = reshape(%4, newshape=[512, 3, 3, 32, 16]) /* ty=Tensor[(512, 3, 3, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 3, 3, 16, 512), int8] */;
  %7 = reshape(%6, newshape=[32, 3, 3, 16, 32, 16]) /* ty=Tensor[(32, 3, 3, 16, 32, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 3, 3, 16, 32, 4, 4]) /* ty=Tensor[(32, 3, 3, 16, 32, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 32, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 512, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 32, 10, 10, 16), 'uint8'), ('TENSOR', (32, 32, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 4.2204 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 50.9119 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.4132 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3055 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1633 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 40.8504 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.2771 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 4.5755 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 47.9463 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.2998 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2222 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1671 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.7426 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.5183 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.504 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 52.2338 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.8225 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.247 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1744 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 44.9028 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.3657 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3505 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 49.4746 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.5181 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2064 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0958 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.4396 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.9709 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3901 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 52.2625 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 21.0529 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2664 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1562 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.0389 us/iter
[00:52:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.0953 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 512, 10, 10), 'uint8'), ('TENSOR', (512, 512, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:33] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 512, 10, 10), int8], %w: Tensor[(512, 512, 3, 3), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 512, 10, 10), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 10, 10, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(512, 3, 3, 512), int8] */;
  %5 = reshape(%4, newshape=[512, 3, 3, 32, 16]) /* ty=Tensor[(512, 3, 3, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 3, 3, 16, 512), int8] */;
  %7 = reshape(%6, newshape=[32, 3, 3, 16, 32, 16]) /* ty=Tensor[(32, 3, 3, 16, 32, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 3, 3, 16, 32, 4, 4]) /* ty=Tensor[(32, 3, 3, 16, 32, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 32, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 512, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 32, 10, 10, 16), 'uint8'), ('TENSOR', (32, 32, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.4562 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 47.8241 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.2253 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2338 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0976 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 44.1947 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.9594 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.2496 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 50.2924 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.4873 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1795 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1429 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.6002 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.4525 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.4561 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 49.1962 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.3817 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1989 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1515 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.8175 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.9117 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.2163 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 52.4412 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 22.5064 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.201 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1743 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 44.1886 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.1222 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3263 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 53.8883 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 22.1489 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2349 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1323 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 45.3214 us/iter
[00:52:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.062 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 512, 10, 10), 'uint8'), ('TENSOR', (512, 512, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:34] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 512, 10, 10), int8], %w: Tensor[(512, 512, 3, 3), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 512, 10, 10), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 10, 10, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(512, 3, 3, 512), int8] */;
  %5 = reshape(%4, newshape=[512, 3, 3, 32, 16]) /* ty=Tensor[(512, 3, 3, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 3, 3, 16, 512), int8] */;
  %7 = reshape(%6, newshape=[32, 3, 3, 16, 32, 16]) /* ty=Tensor[(32, 3, 3, 16, 32, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 3, 3, 16, 32, 4, 4]) /* ty=Tensor[(32, 3, 3, 16, 32, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 32, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 512, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 32, 10, 10, 16), 'uint8'), ('TENSOR', (32, 32, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.5168 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 46.5512 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.2638 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2145 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2436 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 40.2146 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.2883 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3317 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 50.0267 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.4506 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2703 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2252 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.2325 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.4626 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3064 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 48.2791 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.7752 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2112 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1294 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.0202 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.6306 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.5105 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 47.6773 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.6938 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2271 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1702 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.6725 us/iter
[00:52:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.3745 us/iter
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.4889 us/iter
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 55.6348 us/iter
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 21.5084 us/iter
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2581 us/iter
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1593 us/iter
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.9961 us/iter
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.2125 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 512, 10, 10), 'uint8'), ('TENSOR', (512, 512, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:35] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 512, 10, 10), int8], %w: Tensor[(512, 512, 3, 3), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 512, 10, 10), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 10, 10, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(512, 3, 3, 512), int8] */;
  %5 = reshape(%4, newshape=[512, 3, 3, 32, 16]) /* ty=Tensor[(512, 3, 3, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 3, 3, 16, 512), int8] */;
  %7 = reshape(%6, newshape=[32, 3, 3, 16, 32, 16]) /* ty=Tensor[(32, 3, 3, 16, 32, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 3, 3, 16, 32, 4, 4]) /* ty=Tensor[(32, 3, 3, 16, 32, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 32, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 512, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 32, 10, 10, 16), 'uint8'), ('TENSOR', (32, 32, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 4.655 us/iter
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 48.7306 us/iter
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.2497 us/iter
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2671 us/iter
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1327 us/iter
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.5245 us/iter
[00:52:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.3529 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 4.5027 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 48.9976 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 22.0712 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 4.8372 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1847 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.7529 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.5295 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3411 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 50.3706 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.6497 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3463 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2188 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 45.238 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.5219 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3101 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 51.6188 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.4834 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2664 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1965 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.7733 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.5756 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.4339 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 54.0633 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 21.7147 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2451 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1494 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 44.8885 us/iter
[00:52:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.4805 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 512, 10, 10), 'uint8'), ('TENSOR', (512, 512, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:36] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 512, 10, 10), int8], %w: Tensor[(512, 512, 3, 3), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 512, 10, 10), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 10, 10, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(512, 3, 3, 512), int8] */;
  %5 = reshape(%4, newshape=[512, 3, 3, 32, 16]) /* ty=Tensor[(512, 3, 3, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 3, 3, 16, 512), int8] */;
  %7 = reshape(%6, newshape=[32, 3, 3, 16, 32, 16]) /* ty=Tensor[(32, 3, 3, 16, 32, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 3, 3, 16, 32, 4, 4]) /* ty=Tensor[(32, 3, 3, 16, 32, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 32, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 512, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 32, 10, 10, 16), 'uint8'), ('TENSOR', (32, 32, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.4578 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 48.766 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.2888 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2738 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1991 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.1291 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.29 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.5534 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 49.8002 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.3138 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2751 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.212 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.3467 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.3635 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 4.6612 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 52.6411 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 23.7606 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3992 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2755 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 45.0757 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.9788 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.5024 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 55.4888 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 22.0002 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.292 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.213 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 45.5198 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.0675 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.5838 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 59.6917 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 22.8435 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3199 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1663 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.6428 us/iter
[00:52:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.6704 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 512, 10, 10), 'uint8'), ('TENSOR', (512, 512, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:37] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 512, 10, 10), int8], %w: Tensor[(512, 512, 3, 3), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 512, 10, 10), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 10, 10, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(512, 3, 3, 512), int8] */;
  %5 = reshape(%4, newshape=[512, 3, 3, 32, 16]) /* ty=Tensor[(512, 3, 3, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 3, 3, 16, 512), int8] */;
  %7 = reshape(%6, newshape=[32, 3, 3, 16, 32, 16]) /* ty=Tensor[(32, 3, 3, 16, 32, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 3, 3, 16, 32, 4, 4]) /* ty=Tensor[(32, 3, 3, 16, 32, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 32, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 512, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 32, 10, 10, 16), 'uint8'), ('TENSOR', (32, 32, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3374 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 48.7808 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.228 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1748 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0589 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 39.1725 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.7815 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3539 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 49.8388 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.2236 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1898 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1285 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.3987 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.094 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.2315 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 51.1411 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.3258 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1574 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1154 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.9287 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.9261 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.246 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 50.8594 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.3659 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1558 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.044 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.5214 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.9253 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 4.4376 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 49.4623 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.337 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1195 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0521 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.4078 us/iter
[00:52:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.6926 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 512, 10, 10), 'uint8'), ('TENSOR', (512, 512, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:38] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 512, 10, 10), int8], %w: Tensor[(512, 512, 3, 3), int8], %b: Tensor[(1, 512, 1, 1), int32]) -> Tensor[(1, 512, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 512, 10, 10), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 512, 10, 10), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 10, 10, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(512, 3, 3, 512), int8] */;
  %5 = reshape(%4, newshape=[512, 3, 3, 32, 16]) /* ty=Tensor[(512, 3, 3, 32, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(32, 3, 3, 16, 512), int8] */;
  %7 = reshape(%6, newshape=[32, 3, 3, 16, 32, 16]) /* ty=Tensor[(32, 3, 3, 16, 32, 16), int8] */;
  %8 = reshape(%7, newshape=[32, 3, 3, 16, 32, 4, 4]) /* ty=Tensor[(32, 3, 3, 16, 32, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(32, 32, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=512, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(512, 512, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(512), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(512, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(512, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 512, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 32, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 32, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 512, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 32, 10, 10, 16), 'uint8'), ('TENSOR', (32, 32, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.5153 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 49.5496 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.2004 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1923 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1396 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 38.6176 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.8285 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.2696 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 49.6526 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.3052 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2105 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0834 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.1269 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.7063 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 4.488 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 54.9691 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 21.8109 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2361 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1238 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 45.4991 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.9426 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.4284 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 52.5885 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.8726 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2712 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1252 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.8145 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.0022 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.3661 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 50.2654 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 20.4342 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1826 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1494 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.6943 us/iter
[00:52:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.8645 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 16, 16), 'uint8'), ('TENSOR', (160, 192, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:39] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 16, 16), int8], %w: Tensor[(160, 192, 3, 3), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 14, 14), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 3, 3, 160), int8] */;
  %5 = reshape(%4, newshape=[192, 3, 3, 10, 16]) /* ty=Tensor[(192, 3, 3, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 3, 3, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[10, 3, 3, 16, 12, 16]) /* ty=Tensor[(10, 3, 3, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 3, 3, 16, 12, 4, 4]) /* ty=Tensor[(10, 3, 3, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 12, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 192, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 14, 14), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 16, 16, 16), 'uint8'), ('TENSOR', (10, 12, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.51107 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.56215 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.35759 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.07944 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07878 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 28.525 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.96663 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.68815 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.55441 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.37481 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09915 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.06578 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 29.4295 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.11778 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.46171 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.95359 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.49771 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14688 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.048 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 30.3566 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.35424 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.65344 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.74793 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.39941 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10174 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05526 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 29.2796 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.31333 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.54578 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.75378 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.45137 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.15404 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07244 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 29.5541 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.02378 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 16, 16), 'uint8'), ('TENSOR', (160, 192, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:40] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 16, 16), int8], %w: Tensor[(160, 192, 3, 3), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 14, 14), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 3, 3, 160), int8] */;
  %5 = reshape(%4, newshape=[192, 3, 3, 10, 16]) /* ty=Tensor[(192, 3, 3, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 3, 3, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[10, 3, 3, 16, 12, 16]) /* ty=Tensor[(10, 3, 3, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 3, 3, 16, 12, 4, 4]) /* ty=Tensor[(10, 3, 3, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 12, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 192, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 14, 14), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 16, 16, 16), 'uint8'), ('TENSOR', (10, 12, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.6977 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.7074 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.29747 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.17427 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.10207 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 23.9046 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.66803 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 4.43572 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.793 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.34356 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09817 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.084 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 25.8816 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.61556 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.49519 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.81411 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.34515 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14981 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07589 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 27.2497 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.94578 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.53204 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.86167 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.39548 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11493 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0967 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 28.2603 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.784 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.67563 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.94737 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.37163 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11122 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09815 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 26.8624 us/iter
[00:52:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.9597 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 16, 16), 'uint8'), ('TENSOR', (160, 192, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:41] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 16, 16), int8], %w: Tensor[(160, 192, 3, 3), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 14, 14), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 3, 3, 160), int8] */;
  %5 = reshape(%4, newshape=[192, 3, 3, 10, 16]) /* ty=Tensor[(192, 3, 3, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 3, 3, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[10, 3, 3, 16, 12, 16]) /* ty=Tensor[(10, 3, 3, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 3, 3, 16, 12, 4, 4]) /* ty=Tensor[(10, 3, 3, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 12, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 192, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 14, 14), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 16, 16, 16), 'uint8'), ('TENSOR', (10, 12, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.83169 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.92934 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.29824 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.03266 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05793 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 25.3871 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.95266 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.79844 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.70463 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.26115 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08552 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08185 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 24.2597 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.86063 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.67033 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.00489 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.33041 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.06511 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0447 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 29.4041 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.98752 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.42752 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.94089 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.39715 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.075 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07115 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 29.9071 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.43156 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.40924 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.93965 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.32212 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08953 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04576 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 30.6015 us/iter
[00:52:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.29271 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 16, 16), 'uint8'), ('TENSOR', (160, 192, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:42] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 16, 16), int8], %w: Tensor[(160, 192, 3, 3), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 14, 14), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 3, 3, 160), int8] */;
  %5 = reshape(%4, newshape=[192, 3, 3, 10, 16]) /* ty=Tensor[(192, 3, 3, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 3, 3, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[10, 3, 3, 16, 12, 16]) /* ty=Tensor[(10, 3, 3, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 3, 3, 16, 12, 4, 4]) /* ty=Tensor[(10, 3, 3, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 12, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 192, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 14, 14), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 16, 16, 16), 'uint8'), ('TENSOR', (10, 12, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.66884 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.50911 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.31484 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 3.45889 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.14284 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 24.0771 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.79142 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.61679 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.5131 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.34169 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12876 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08572 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 28.1163 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.72528 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.64122 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.02678 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.50144 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10883 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09939 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 28.1207 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.0495 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.62111 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.68544 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.36644 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14333 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09744 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 27.3521 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.27828 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.54444 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.56756 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.33119 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13563 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09493 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 31.5144 us/iter
[00:52:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.08193 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 16, 16), 'uint8'), ('TENSOR', (160, 192, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:43] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 16, 16), int8], %w: Tensor[(160, 192, 3, 3), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 14, 14), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 3, 3, 160), int8] */;
  %5 = reshape(%4, newshape=[192, 3, 3, 10, 16]) /* ty=Tensor[(192, 3, 3, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 3, 3, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[10, 3, 3, 16, 12, 16]) /* ty=Tensor[(10, 3, 3, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 3, 3, 16, 12, 4, 4]) /* ty=Tensor[(10, 3, 3, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 12, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 192, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 14, 14), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 16, 16, 16), 'uint8'), ('TENSOR', (10, 12, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.71721 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.77079 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.37372 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11869 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09779 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 27.0324 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.25186 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.58333 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.68837 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.33759 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1227 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.11256 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 28.8245 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.13978 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.77693 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.84372 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.3909 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13059 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08286 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 26.7581 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.30783 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.62871 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.37735 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.49329 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.16412 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.11782 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 29.5703 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.421 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.92683 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.81452 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.72483 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14324 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.11379 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 23.1361 us/iter
[00:52:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.5349 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 16, 16), 'uint8'), ('TENSOR', (160, 192, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:44] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 16, 16), int8], %w: Tensor[(160, 192, 3, 3), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 14, 14), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 3, 3, 160), int8] */;
  %5 = reshape(%4, newshape=[192, 3, 3, 10, 16]) /* ty=Tensor[(192, 3, 3, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 3, 3, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[10, 3, 3, 16, 12, 16]) /* ty=Tensor[(10, 3, 3, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 3, 3, 16, 12, 4, 4]) /* ty=Tensor[(10, 3, 3, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 12, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 192, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 14, 14), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 16, 16, 16), 'uint8'), ('TENSOR', (10, 12, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.68148 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.63145 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.41986 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.16621 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.12852 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 25.1534 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.45924 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.61907 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.64741 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.43715 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.17626 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.16219 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 28.2439 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.48067 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.68389 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.82089 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.389 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.17981 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.14052 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 28.8842 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.2447 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.64652 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.16663 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.48426 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.18878 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.13241 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 29.346 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.00541 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.62711 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.82985 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.42922 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.21244 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.18748 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 28.1862 us/iter
[00:52:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.96159 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 16, 16), 'uint8'), ('TENSOR', (160, 192, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:44] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 16, 16), int8], %w: Tensor[(160, 192, 3, 3), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 14, 14), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 3, 3, 160), int8] */;
  %5 = reshape(%4, newshape=[192, 3, 3, 10, 16]) /* ty=Tensor[(192, 3, 3, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 3, 3, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[10, 3, 3, 16, 12, 16]) /* ty=Tensor[(10, 3, 3, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 3, 3, 16, 12, 4, 4]) /* ty=Tensor[(10, 3, 3, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 12, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 192, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 14, 14), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 16, 16, 16), 'uint8'), ('TENSOR', (10, 12, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.93153 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.672 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.29626 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.01437 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.00742 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 24.462 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.33684 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.71644 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.71163 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.27463 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.05056 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04226 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 27.386 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.81167 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.91437 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.6093 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.44511 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14096 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09119 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 22.6485 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.40407 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.61222 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.03328 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.42322 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10206 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05822 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 27.5189 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.72022 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.63367 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.99572 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.42528 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14983 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05244 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 27.4247 us/iter
[00:52:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.66833 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 16, 16), 'uint8'), ('TENSOR', (160, 192, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:45] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 16, 16), int8], %w: Tensor[(160, 192, 3, 3), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 14, 14), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 3, 3, 160), int8] */;
  %5 = reshape(%4, newshape=[192, 3, 3, 10, 16]) /* ty=Tensor[(192, 3, 3, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 3, 3, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[10, 3, 3, 16, 12, 16]) /* ty=Tensor[(10, 3, 3, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 3, 3, 16, 12, 4, 4]) /* ty=Tensor[(10, 3, 3, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 12, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 192, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 14, 14), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 16, 16, 16), 'uint8'), ('TENSOR', (10, 12, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 4.93844 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.99759 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.58093 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.22074 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.13459 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 26.6095 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.36167 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.7757 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.15841 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.4813 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.18078 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.14293 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 28.096 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.3813 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.818 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.86522 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.09007 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.25511 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.18711 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 27.4532 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.80433 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.68952 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.67574 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.46215 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.18563 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.13352 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 28.25 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.61085 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.83917 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.71306 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.60006 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.22622 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.20694 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 25.1893 us/iter
[00:52:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.75244 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 16, 16), 'uint8'), ('TENSOR', (160, 192, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:46] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 16, 16), int8], %w: Tensor[(160, 192, 3, 3), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 14, 14), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 3, 3, 160), int8] */;
  %5 = reshape(%4, newshape=[192, 3, 3, 10, 16]) /* ty=Tensor[(192, 3, 3, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 3, 3, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[10, 3, 3, 16, 12, 16]) /* ty=Tensor[(10, 3, 3, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 3, 3, 16, 12, 4, 4]) /* ty=Tensor[(10, 3, 3, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 12, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 192, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 14, 14), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 16, 16, 16), 'uint8'), ('TENSOR', (10, 12, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.48348 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.37148 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.34414 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09245 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04307 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 26.3606 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.17466 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 4.01938 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.77214 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.34517 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11376 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09045 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 27.6719 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.27062 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.50024 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.69168 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.30264 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.15612 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.10904 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 31.4614 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.09072 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.45119 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.98822 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.39852 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.16252 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09137 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 30.5266 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.29948 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.59994 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.91953 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.46065 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.168 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.12376 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 29.9334 us/iter
[00:52:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.28753 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 16, 16), 'uint8'), ('TENSOR', (160, 192, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:47] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 16, 16), int8], %w: Tensor[(160, 192, 3, 3), int8], %b: Tensor[(1, 160, 1, 1), int32]) -> Tensor[(1, 160, 14, 14), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 16, 16), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 16, 16), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 16, 16, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 3, 3, 160), int8] */;
  %5 = reshape(%4, newshape=[192, 3, 3, 10, 16]) /* ty=Tensor[(192, 3, 3, 10, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(10, 3, 3, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[10, 3, 3, 16, 12, 16]) /* ty=Tensor[(10, 3, 3, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[10, 3, 3, 16, 12, 4, 4]) /* ty=Tensor[(10, 3, 3, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(10, 12, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=160, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(160, 192, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(160), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(160, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(160, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 160, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 10, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 10, 14, 14, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 160, 14, 14), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 16, 16, 16), 'uint8'), ('TENSOR', (10, 12, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.70517 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.67962 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.27814 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.07245 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.03372 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 26.3856 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 7.16655 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.77017 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.83883 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.33317 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.132 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05339 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 27.4695 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.34878 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.77652 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0723 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.36544 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12952 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08585 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 27.1228 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.37944 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.84804 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.69422 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.28756 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10133 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.52048 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 27.377 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.61385 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.69626 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.86537 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.3523 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.65489 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.163 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 28.4097 us/iter
[00:52:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.33937 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 65, 65), 'uint8'), ('TENSOR', (128, 64, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:48] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 65, 65), int8], %w: Tensor[(128, 64, 3, 3), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 3, 3, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 3, 3, 8, 16]) /* ty=Tensor[(64, 3, 3, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 3, 3, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 3, 3, 16, 4, 16]) /* ty=Tensor[(8, 3, 3, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 3, 3, 16, 4, 4, 4]) /* ty=Tensor[(8, 3, 3, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 65, 65, 16), 'uint8'), ('TENSOR', (8, 4, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.3659 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7601 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9008 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1273 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1413 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 139.262 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.5378 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.3537 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7078 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8615 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1218 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1174 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 139.893 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.6896 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.1832 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7449 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.895 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1723 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0885 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 139.085 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.7204 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.1607 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8645 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8831 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1599 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1263 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 139.556 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.2749 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.3938 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.9273 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9968 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2976 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1712 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 142.02 us/iter
[00:52:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 40.7748 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 65, 65), 'uint8'), ('TENSOR', (128, 64, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:49] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 65, 65), int8], %w: Tensor[(128, 64, 3, 3), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 3, 3, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 3, 3, 8, 16]) /* ty=Tensor[(64, 3, 3, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 3, 3, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 3, 3, 16, 4, 16]) /* ty=Tensor[(8, 3, 3, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 3, 3, 16, 4, 4, 4]) /* ty=Tensor[(8, 3, 3, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 65, 65, 16), 'uint8'), ('TENSOR', (8, 4, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.3349 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.89 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9841 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2559 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1686 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 135.793 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.7967 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.3647 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.735 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9592 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1433 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1491 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 134.221 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.0936 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.3982 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8671 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9576 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1523 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1464 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 139.647 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.1892 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.4683 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7019 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9523 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1572 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1486 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 137.789 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.3026 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.5591 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8251 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9394 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1523 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1134 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 140.12 us/iter
[00:52:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 41.4732 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 65, 65), 'uint8'), ('TENSOR', (128, 64, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:50] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 65, 65), int8], %w: Tensor[(128, 64, 3, 3), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 3, 3, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 3, 3, 8, 16]) /* ty=Tensor[(64, 3, 3, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 3, 3, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 3, 3, 16, 4, 16]) /* ty=Tensor[(8, 3, 3, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 3, 3, 16, 4, 4, 4]) /* ty=Tensor[(8, 3, 3, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 65, 65, 16), 'uint8'), ('TENSOR', (8, 4, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.6479 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 4.0869 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 3.0205 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.151 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1166 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 114.557 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.5517 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.1255 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7604 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9525 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1757 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1267 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 123.021 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.521 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.3331 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7889 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9757 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.119 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1217 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 121.22 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 39.4638 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.4189 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.9637 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9506 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1542 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1159 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 121.939 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 40.1251 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.1756 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8466 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9185 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1663 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1392 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 119.618 us/iter
[00:52:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.8338 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 65, 65), 'uint8'), ('TENSOR', (128, 64, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:51] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 65, 65), int8], %w: Tensor[(128, 64, 3, 3), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 3, 3, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 3, 3, 8, 16]) /* ty=Tensor[(64, 3, 3, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 3, 3, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 3, 3, 16, 4, 16]) /* ty=Tensor[(8, 3, 3, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 3, 3, 16, 4, 4, 4]) /* ty=Tensor[(8, 3, 3, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 65, 65, 16), 'uint8'), ('TENSOR', (8, 4, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.2664 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8163 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9226 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1504 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1547 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 140.495 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 39.029 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.2738 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.715 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8393 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1649 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1503 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 140.599 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.1971 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.1963 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7736 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8493 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1871 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1055 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 139.834 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.1917 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.1806 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 5.3481 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 3.0704 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1996 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1594 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 133.046 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.7193 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.254 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.9949 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9532 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2424 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1935 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 139.319 us/iter
[00:52:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.5917 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 65, 65), 'uint8'), ('TENSOR', (128, 64, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:52] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 65, 65), int8], %w: Tensor[(128, 64, 3, 3), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 3, 3, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 3, 3, 8, 16]) /* ty=Tensor[(64, 3, 3, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 3, 3, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 3, 3, 16, 4, 16]) /* ty=Tensor[(8, 3, 3, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 3, 3, 16, 4, 4, 4]) /* ty=Tensor[(8, 3, 3, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 65, 65, 16), 'uint8'), ('TENSOR', (8, 4, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.5804 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.9247 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 3.0151 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3615 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3002 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 134.411 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.2989 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.8093 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 4.9822 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 3.1999 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2895 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2283 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 133.718 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 45.9186 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.244 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8254 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.999 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2664 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2569 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 135.295 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.37 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.411 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7857 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9126 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2502 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2112 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 138.855 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.323 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.4138 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8861 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9962 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2646 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2078 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 131.105 us/iter
[00:52:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.3985 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 65, 65), 'uint8'), ('TENSOR', (128, 64, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:53] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 65, 65), int8], %w: Tensor[(128, 64, 3, 3), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 3, 3, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 3, 3, 8, 16]) /* ty=Tensor[(64, 3, 3, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 3, 3, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 3, 3, 16, 4, 16]) /* ty=Tensor[(8, 3, 3, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 3, 3, 16, 4, 4, 4]) /* ty=Tensor[(8, 3, 3, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 65, 65, 16), 'uint8'), ('TENSOR', (8, 4, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.4148 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.9271 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9901 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2049 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1435 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 134.329 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.6321 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.3097 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7371 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9096 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1328 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0078 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 135.979 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.5234 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.3431 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7594 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8675 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1168 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1111 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 133.222 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.4977 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.4026 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 4.0814 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8851 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.171 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0459 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 136.278 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.4467 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.6029 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.72 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9333 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1046 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0523 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 138.11 us/iter
[00:52:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.7514 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 65, 65), 'uint8'), ('TENSOR', (128, 64, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:54] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 65, 65), int8], %w: Tensor[(128, 64, 3, 3), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 3, 3, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 3, 3, 8, 16]) /* ty=Tensor[(64, 3, 3, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 3, 3, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 3, 3, 16, 4, 16]) /* ty=Tensor[(8, 3, 3, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 3, 3, 16, 4, 4, 4]) /* ty=Tensor[(8, 3, 3, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 65, 65, 16), 'uint8'), ('TENSOR', (8, 4, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.203 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7907 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9698 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1838 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1506 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 134.235 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.3831 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.293 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7846 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9831 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1697 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1168 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 131.69 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.8485 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.4373 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 4.0024 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 3.047 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2413 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2579 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 134.253 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 40.0927 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.2251 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.9139 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9882 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2092 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1548 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 138.169 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.3426 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.5651 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7091 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9696 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1776 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1637 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 133.521 us/iter
[00:52:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.7112 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 65, 65), 'uint8'), ('TENSOR', (128, 64, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:55] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 65, 65), int8], %w: Tensor[(128, 64, 3, 3), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 3, 3, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 3, 3, 8, 16]) /* ty=Tensor[(64, 3, 3, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 3, 3, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 3, 3, 16, 4, 16]) /* ty=Tensor[(8, 3, 3, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 3, 3, 16, 4, 4, 4]) /* ty=Tensor[(8, 3, 3, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 65, 65, 16), 'uint8'), ('TENSOR', (8, 4, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.4225 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7389 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9054 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.097 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0863 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 133.234 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.63 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.3037 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.6964 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9034 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1312 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0822 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 137.3 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.9798 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.2012 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7626 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9467 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1404 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0588 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 139.625 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.0511 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.2015 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7364 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9216 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1158 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1472 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 137.027 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.1694 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.281 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7284 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8811 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1494 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1371 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 137.505 us/iter
[00:52:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.7083 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 65, 65), 'uint8'), ('TENSOR', (128, 64, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:57] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 65, 65), int8], %w: Tensor[(128, 64, 3, 3), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 3, 3, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 3, 3, 8, 16]) /* ty=Tensor[(64, 3, 3, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 3, 3, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 3, 3, 16, 4, 16]) /* ty=Tensor[(8, 3, 3, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 3, 3, 16, 4, 4, 4]) /* ty=Tensor[(8, 3, 3, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 65, 65, 16), 'uint8'), ('TENSOR', (8, 4, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.4458 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7194 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.864 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0961 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0787 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 136.132 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.3848 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.0888 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7372 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8423 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0491 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0577 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 141.251 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.4572 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.203 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7945 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8791 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1335 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0408 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 140.154 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.1053 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.417 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.6909 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.844 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0493 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0715 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 140.375 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.2352 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.3432 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7887 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8818 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0607 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0204 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 139.643 us/iter
[00:52:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.7634 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 65, 65), 'uint8'), ('TENSOR', (128, 64, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:58] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 64, 65, 65), int8], %w: Tensor[(128, 64, 3, 3), int8], %b: Tensor[(1, 128, 1, 1), int32]) -> Tensor[(1, 128, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 64, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 64, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(64, 3, 3, 128), int8] */;
  %5 = reshape(%4, newshape=[64, 3, 3, 8, 16]) /* ty=Tensor[(64, 3, 3, 8, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(8, 3, 3, 16, 64), int8] */;
  %7 = reshape(%6, newshape=[8, 3, 3, 16, 4, 16]) /* ty=Tensor[(8, 3, 3, 16, 4, 16), int8] */;
  %8 = reshape(%7, newshape=[8, 3, 3, 16, 4, 4, 4]) /* ty=Tensor[(8, 3, 3, 16, 4, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(8, 4, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=128, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(128, 64, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(128), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(128, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(128, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 128, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 8, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 128, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 65, 65, 16), 'uint8'), ('TENSOR', (8, 4, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.4489 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.8254 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.949 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.164 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1746 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 135.336 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 40.4045 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.3111 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.6643 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9006 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1068 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0784 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 136.678 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 37.732 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.319 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 4.0776 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9645 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.16 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1079 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 141.676 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 40.5417 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.2771 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.9239 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.9272 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1277 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0972 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 141.751 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 40.8389 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 8.1883 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 3.7609 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8579 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.084 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1553 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 140.34 us/iter
[00:52:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 38.8375 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 320, 65, 65), 'uint8'), ('TENSOR', (384, 320, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:59] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 320, 65, 65), int8], %w: Tensor[(384, 320, 3, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 320, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 20, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(320, 3, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[320, 3, 3, 24, 16]) /* ty=Tensor[(320, 3, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 3, 3, 16, 320), int8] */;
  %7 = reshape(%6, newshape=[24, 3, 3, 16, 20, 16]) /* ty=Tensor[(24, 3, 3, 16, 20, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 3, 3, 16, 20, 4, 4]) /* ty=Tensor[(24, 3, 3, 16, 20, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 20, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 320, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 20, 65, 65, 16), 'uint8'), ('TENSOR', (24, 20, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.6387 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.7403 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.767 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4196 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3633 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1006.9 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 67.8873 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.6311 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.4005 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.8871 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4602 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4977 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1006.14 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.2039 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.8531 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.2868 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.8119 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4898 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4381 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1005.1 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 69.3164 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5868 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 27.6272 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 11.4226 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4672 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4989 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1022.13 us/iter
[00:52:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.1489 us/iter
[00:53:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.8498 us/iter
[00:53:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.3335 us/iter
[00:53:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7247 us/iter
[00:53:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3974 us/iter
[00:53:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4098 us/iter
[00:53:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1029.41 us/iter
[00:53:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.0061 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 320, 65, 65), 'uint8'), ('TENSOR', (384, 320, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:00] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 320, 65, 65), int8], %w: Tensor[(384, 320, 3, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 320, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 20, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(320, 3, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[320, 3, 3, 24, 16]) /* ty=Tensor[(320, 3, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 3, 3, 16, 320), int8] */;
  %7 = reshape(%6, newshape=[24, 3, 3, 16, 20, 16]) /* ty=Tensor[(24, 3, 3, 16, 20, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 3, 3, 16, 20, 4, 4]) /* ty=Tensor[(24, 3, 3, 16, 20, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 20, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 320, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 20, 65, 65, 16), 'uint8'), ('TENSOR', (24, 20, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5201 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.2536 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.6579 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4502 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.28 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1005.19 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.1256 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.6306 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.9345 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 12.7935 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.7888 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3989 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1003.38 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.6 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5942 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 26.1529 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 11.1458 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.536 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3939 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1011.76 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 71.6792 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.7785 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.8483 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7693 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4935 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3224 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1013.32 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 69.8386 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.4954 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.9628 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7933 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4826 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3111 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1004.02 us/iter
[00:53:01] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 69.087 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 320, 65, 65), 'uint8'), ('TENSOR', (384, 320, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:02] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 320, 65, 65), int8], %w: Tensor[(384, 320, 3, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 320, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 20, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(320, 3, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[320, 3, 3, 24, 16]) /* ty=Tensor[(320, 3, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 3, 3, 16, 320), int8] */;
  %7 = reshape(%6, newshape=[24, 3, 3, 16, 20, 16]) /* ty=Tensor[(24, 3, 3, 16, 20, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 3, 3, 16, 20, 4, 4]) /* ty=Tensor[(24, 3, 3, 16, 20, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 20, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 320, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 20, 65, 65, 16), 'uint8'), ('TENSOR', (24, 20, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5984 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.4389 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 11.0863 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3355 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2742 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1013.48 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.1629 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5881 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 28.586 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 11.2209 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4349 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3346 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1009.28 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 71.3411 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 25.6495 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.3471 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7406 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.392 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3292 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1010.12 us/iter
[00:53:02] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 71.7288 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.9828 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 27.2502 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 11.2483 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4365 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.323 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1036.04 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 72.1087 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.4057 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.2171 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.824 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3913 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2962 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1028.02 us/iter
[00:53:03] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 69.0421 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 320, 65, 65), 'uint8'), ('TENSOR', (384, 320, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:03] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 320, 65, 65), int8], %w: Tensor[(384, 320, 3, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 320, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 20, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(320, 3, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[320, 3, 3, 24, 16]) /* ty=Tensor[(320, 3, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 3, 3, 16, 320), int8] */;
  %7 = reshape(%6, newshape=[24, 3, 3, 16, 20, 16]) /* ty=Tensor[(24, 3, 3, 16, 20, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 3, 3, 16, 20, 4, 4]) /* ty=Tensor[(24, 3, 3, 16, 20, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 20, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 320, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 20, 65, 65, 16), 'uint8'), ('TENSOR', (24, 20, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.7097 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.1933 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7178 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.53 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2861 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 997.056 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.7828 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 25.1503 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 26.0434 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 11.1938 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4899 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2996 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 998.838 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 71.171 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.6 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.8829 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.871 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4077 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3702 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 998.066 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.142 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 27.7159 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.5162 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.8189 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.5423 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3843 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1041.16 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 71.0181 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.647 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.7024 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 12.292 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.6963 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3543 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1013.1 us/iter
[00:53:04] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 67.868 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 320, 65, 65), 'uint8'), ('TENSOR', (384, 320, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:05] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 320, 65, 65), int8], %w: Tensor[(384, 320, 3, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 320, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 20, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(320, 3, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[320, 3, 3, 24, 16]) /* ty=Tensor[(320, 3, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 3, 3, 16, 320), int8] */;
  %7 = reshape(%6, newshape=[24, 3, 3, 16, 20, 16]) /* ty=Tensor[(24, 3, 3, 16, 20, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 3, 3, 16, 20, 4, 4]) /* ty=Tensor[(24, 3, 3, 16, 20, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 20, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 320, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 20, 65, 65, 16), 'uint8'), ('TENSOR', (24, 20, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.6543 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.6335 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.6915 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3169 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3015 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1002.31 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 67.8447 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.6538 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.2546 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.6923 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3718 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2929 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1001.77 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.3287 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.7888 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.2202 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7991 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3426 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2589 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1011.07 us/iter
[00:53:05] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.713 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.4409 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 28.5116 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 13.5913 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4332 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3557 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1016.9 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.1744 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.6223 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.237 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.8484 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3876 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3046 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1025.62 us/iter
[00:53:06] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 69.3566 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 320, 65, 65), 'uint8'), ('TENSOR', (384, 320, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:06] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 320, 65, 65), int8], %w: Tensor[(384, 320, 3, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 320, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 20, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(320, 3, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[320, 3, 3, 24, 16]) /* ty=Tensor[(320, 3, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 3, 3, 16, 320), int8] */;
  %7 = reshape(%6, newshape=[24, 3, 3, 16, 20, 16]) /* ty=Tensor[(24, 3, 3, 16, 20, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 3, 3, 16, 20, 4, 4]) /* ty=Tensor[(24, 3, 3, 16, 20, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 20, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 320, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 20, 65, 65, 16), 'uint8'), ('TENSOR', (24, 20, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.6658 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.2512 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.768 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4908 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3863 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1003.79 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 67.8111 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.7158 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.769 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7893 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4368 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3974 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 999.961 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.3416 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.6553 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.8378 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.8656 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4808 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3707 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1001.41 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.4017 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 26.8107 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.9239 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7994 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4107 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3985 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1017.65 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.3319 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.6032 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.2356 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 12.859 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.6085 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4547 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1016.24 us/iter
[00:53:07] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.4852 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 320, 65, 65), 'uint8'), ('TENSOR', (384, 320, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:08] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 320, 65, 65), int8], %w: Tensor[(384, 320, 3, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 320, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 20, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(320, 3, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[320, 3, 3, 24, 16]) /* ty=Tensor[(320, 3, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 3, 3, 16, 320), int8] */;
  %7 = reshape(%6, newshape=[24, 3, 3, 16, 20, 16]) /* ty=Tensor[(24, 3, 3, 16, 20, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 3, 3, 16, 20, 4, 4]) /* ty=Tensor[(24, 3, 3, 16, 20, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 20, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 320, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 20, 65, 65, 16), 'uint8'), ('TENSOR', (24, 20, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5509 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.3419 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.8179 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.5701 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3579 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1020.89 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.4211 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.7388 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.888 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.8369 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4941 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3719 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1006.28 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 70.5467 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.4582 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.952 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.8798 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4683 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3267 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1003.78 us/iter
[00:53:08] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 71.9685 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 25.584 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 28.9687 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 11.2777 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4836 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4204 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1010.48 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 73.3336 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.6708 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.9034 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.8138 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4685 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3913 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1018.41 us/iter
[00:53:09] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 69.0531 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 320, 65, 65), 'uint8'), ('TENSOR', (384, 320, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:09] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 320, 65, 65), int8], %w: Tensor[(384, 320, 3, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 320, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 20, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(320, 3, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[320, 3, 3, 24, 16]) /* ty=Tensor[(320, 3, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 3, 3, 16, 320), int8] */;
  %7 = reshape(%6, newshape=[24, 3, 3, 16, 20, 16]) /* ty=Tensor[(24, 3, 3, 16, 20, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 3, 3, 16, 20, 4, 4]) /* ty=Tensor[(24, 3, 3, 16, 20, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 20, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 320, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 20, 65, 65, 16), 'uint8'), ('TENSOR', (24, 20, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 26.9174 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.5348 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.8616 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4276 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4031 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1002.87 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 67.8639 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5895 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.1564 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.936 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.5152 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4898 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 998.612 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.3659 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 27.3211 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 28.6446 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 11.2037 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.5085 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4598 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1014.12 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.5738 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5513 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.3348 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.8498 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4775 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4292 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1015.62 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.661 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 26.2402 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 27.2241 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 11.4312 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.5013 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4463 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1015.29 us/iter
[00:53:10] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.3315 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 320, 65, 65), 'uint8'), ('TENSOR', (384, 320, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:11] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 320, 65, 65), int8], %w: Tensor[(384, 320, 3, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 320, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 20, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(320, 3, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[320, 3, 3, 24, 16]) /* ty=Tensor[(320, 3, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 3, 3, 16, 320), int8] */;
  %7 = reshape(%6, newshape=[24, 3, 3, 16, 20, 16]) /* ty=Tensor[(24, 3, 3, 16, 20, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 3, 3, 16, 20, 4, 4]) /* ty=Tensor[(24, 3, 3, 16, 20, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 20, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 320, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 20, 65, 65, 16), 'uint8'), ('TENSOR', (24, 20, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5414 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.4054 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7176 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3923 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.316 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1013.7 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.0565 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5575 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.9285 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7601 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3651 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.4295 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1013.2 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.648 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.9013 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 26.396 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 11.2038 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3628 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.46 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1014.95 us/iter
[00:53:11] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.7651 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.601 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.0625 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7325 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3727 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3976 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1001.2 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.6525 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.8329 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 26.281 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 11.1467 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4043 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3869 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1012.58 us/iter
[00:53:12] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 71.8506 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 320, 65, 65), 'uint8'), ('TENSOR', (384, 320, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:12] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 320, 65, 65), int8], %w: Tensor[(384, 320, 3, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 63, 63), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 320, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 320, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 20, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(320, 3, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[320, 3, 3, 24, 16]) /* ty=Tensor[(320, 3, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 3, 3, 16, 320), int8] */;
  %7 = reshape(%6, newshape=[24, 3, 3, 16, 20, 16]) /* ty=Tensor[(24, 3, 3, 16, 20, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 3, 3, 16, 20, 4, 4]) /* ty=Tensor[(24, 3, 3, 16, 20, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 20, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 320, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 63, 63, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 63, 63), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 20, 65, 65, 16), 'uint8'), ('TENSOR', (24, 20, 3, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5549 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.5496 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.6784 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4031 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3362 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1008.32 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 67.9948 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5516 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.8853 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7532 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3952 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2616 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1001.67 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 71.0321 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5432 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 24.8685 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7223 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.449 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2883 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1001.71 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.3221 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5249 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.8083 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 11.1587 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4469 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3602 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1016.58 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.5127 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 24.5554 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 25.0107 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 10.7086 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3969 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3006 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 1018.63 us/iter
[00:53:13] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 68.2053 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 384, 8, 8), 'uint8'), ('TENSOR', (384, 384, 1, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:14] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 384, 8, 8), int8], %w: Tensor[(384, 384, 1, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 8, 6), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 384, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(384, 1, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[384, 1, 3, 24, 16]) /* ty=Tensor[(384, 1, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 1, 3, 16, 384), int8] */;
  %7 = reshape(%6, newshape=[24, 1, 3, 16, 24, 16]) /* ty=Tensor[(24, 1, 3, 16, 24, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 1, 3, 16, 24, 4, 4]) /* ty=Tensor[(24, 1, 3, 16, 24, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 24, 1, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 384, 1, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 8, 6), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 24, 8, 8, 16), 'uint8'), ('TENSOR', (24, 24, 1, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.44903 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.5291 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.74727 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09722 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07503 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.7368 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.28624 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.46676 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.5545 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.71686 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.07546 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05127 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.4759 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.27522 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.553 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.5169 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.81978 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14222 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.10274 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.8493 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.83574 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.48184 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.1085 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.74243 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10476 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07497 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.4129 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.70373 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.93514 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.2111 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.76595 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12368 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1003 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.2713 us/iter
[00:53:14] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.29141 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 384, 8, 8), 'uint8'), ('TENSOR', (384, 384, 1, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:14] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 384, 8, 8), int8], %w: Tensor[(384, 384, 1, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 8, 6), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 384, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(384, 1, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[384, 1, 3, 24, 16]) /* ty=Tensor[(384, 1, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 1, 3, 16, 384), int8] */;
  %7 = reshape(%6, newshape=[24, 1, 3, 16, 24, 16]) /* ty=Tensor[(24, 1, 3, 16, 24, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 1, 3, 16, 24, 4, 4]) /* ty=Tensor[(24, 1, 3, 16, 24, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 24, 1, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 384, 1, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 8, 6), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 24, 8, 8, 16), 'uint8'), ('TENSOR', (24, 24, 1, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.48929 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.5683 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.71113 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.05968 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.03955 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.8245 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.51139 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.41522 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.5782 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.69935 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12751 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07649 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.7149 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.23249 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.49863 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.2158 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.76555 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1225 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05747 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.8048 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.72755 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.52534 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.0779 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.76097 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14247 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.06324 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.1211 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.80616 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.45937 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.8795 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.74177 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10137 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04509 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.7582 us/iter
[00:53:15] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.75886 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 384, 8, 8), 'uint8'), ('TENSOR', (384, 384, 1, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:15] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 384, 8, 8), int8], %w: Tensor[(384, 384, 1, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 8, 6), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 384, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(384, 1, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[384, 1, 3, 24, 16]) /* ty=Tensor[(384, 1, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 1, 3, 16, 384), int8] */;
  %7 = reshape(%6, newshape=[24, 1, 3, 16, 24, 16]) /* ty=Tensor[(24, 1, 3, 16, 24, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 1, 3, 16, 24, 4, 4]) /* ty=Tensor[(24, 1, 3, 16, 24, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 24, 1, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 384, 1, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 8, 6), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 24, 8, 8, 16), 'uint8'), ('TENSOR', (24, 24, 1, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.4337 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.3335 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.71122 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.07495 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05516 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.5382 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.66041 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.46808 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.3248 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.72546 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09519 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04946 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.3862 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.46751 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.55957 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.9268 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.96393 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14293 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1155 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.108 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.1383 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.526 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.5739 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.75805 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12984 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07386 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.1932 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.51484 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.48024 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.6823 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.71997 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11862 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09073 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.2099 us/iter
[00:53:16] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.37051 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 384, 8, 8), 'uint8'), ('TENSOR', (384, 384, 1, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:16] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 384, 8, 8), int8], %w: Tensor[(384, 384, 1, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 8, 6), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 384, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(384, 1, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[384, 1, 3, 24, 16]) /* ty=Tensor[(384, 1, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 1, 3, 16, 384), int8] */;
  %7 = reshape(%6, newshape=[24, 1, 3, 16, 24, 16]) /* ty=Tensor[(24, 1, 3, 16, 24, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 1, 3, 16, 24, 4, 4]) /* ty=Tensor[(24, 1, 3, 16, 24, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 24, 1, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 384, 1, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 8, 6), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 24, 8, 8, 16), 'uint8'), ('TENSOR', (24, 24, 1, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.47087 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.6129 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.70379 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08924 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.03945 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.2384 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.46511 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.48449 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.7416 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.09776 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08105 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.06227 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.1295 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.36135 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.44743 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.8761 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.71678 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09514 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.03203 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.2895 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.66181 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.52457 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.1069 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.07043 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10549 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0877 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.9608 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.40997 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.49989 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.1306 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.72123 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1172 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.02466 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.6039 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.07566 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 384, 8, 8), 'uint8'), ('TENSOR', (384, 384, 1, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:17] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 384, 8, 8), int8], %w: Tensor[(384, 384, 1, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 8, 6), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 384, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(384, 1, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[384, 1, 3, 24, 16]) /* ty=Tensor[(384, 1, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 1, 3, 16, 384), int8] */;
  %7 = reshape(%6, newshape=[24, 1, 3, 16, 24, 16]) /* ty=Tensor[(24, 1, 3, 16, 24, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 1, 3, 16, 24, 4, 4]) /* ty=Tensor[(24, 1, 3, 16, 24, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 24, 1, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 384, 1, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 8, 6), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 24, 8, 8, 16), 'uint8'), ('TENSOR', (24, 24, 1, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 3.53479 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.8782 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.77329 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.16917 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09225 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.4339 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.48567 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.39581 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.5294 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.71349 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.07978 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.03938 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.4815 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.37359 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.53392 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.9895 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.72105 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11646 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08616 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.5992 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.93505 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.43341 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 12.0155 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.708 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10754 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07765 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.0902 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.32854 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.45954 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.4134 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.72826 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.17483 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0484 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.1129 us/iter
[00:53:17] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.53397 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 384, 8, 8), 'uint8'), ('TENSOR', (384, 384, 1, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:18] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 384, 8, 8), int8], %w: Tensor[(384, 384, 1, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 8, 6), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 384, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(384, 1, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[384, 1, 3, 24, 16]) /* ty=Tensor[(384, 1, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 1, 3, 16, 384), int8] */;
  %7 = reshape(%6, newshape=[24, 1, 3, 16, 24, 16]) /* ty=Tensor[(24, 1, 3, 16, 24, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 1, 3, 16, 24, 4, 4]) /* ty=Tensor[(24, 1, 3, 16, 24, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 24, 1, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 384, 1, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 8, 6), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 24, 8, 8, 16), 'uint8'), ('TENSOR', (24, 24, 1, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.44254 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.4821 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.32849 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.02127 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 1.96754 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.2577 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.25795 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.46803 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.4575 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.02871 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.00961 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 1.98995 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.5363 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.47768 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.46389 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.264 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.0615 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.66561 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.02258 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.5529 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.3165 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.52174 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.9491 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.71089 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.01784 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 1.99137 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.8643 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.957 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.48546 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.7963 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.69805 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.03284 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.02865 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.5933 us/iter
[00:53:18] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.75314 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 384, 8, 8), 'uint8'), ('TENSOR', (384, 384, 1, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:19] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 384, 8, 8), int8], %w: Tensor[(384, 384, 1, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 8, 6), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 384, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(384, 1, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[384, 1, 3, 24, 16]) /* ty=Tensor[(384, 1, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 1, 3, 16, 384), int8] */;
  %7 = reshape(%6, newshape=[24, 1, 3, 16, 24, 16]) /* ty=Tensor[(24, 1, 3, 16, 24, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 1, 3, 16, 24, 4, 4]) /* ty=Tensor[(24, 1, 3, 16, 24, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 24, 1, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 384, 1, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 8, 6), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 24, 8, 8, 16), 'uint8'), ('TENSOR', (24, 24, 1, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.58622 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.1277 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.77765 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.16011 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.14165 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.8893 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.24284 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.60419 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.5752 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.82108 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1753 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.12668 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.3205 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.033 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.62265 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.2362 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.85103 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.21876 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.16527 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.151 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.09751 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.68097 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.4427 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.86363 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.54254 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2028 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.1852 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.12543 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.64975 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.7757 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.80387 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.19419 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.17484 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.057 us/iter
[00:53:19] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 6.99728 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 384, 8, 8), 'uint8'), ('TENSOR', (384, 384, 1, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:19] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 384, 8, 8), int8], %w: Tensor[(384, 384, 1, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 8, 6), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 384, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(384, 1, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[384, 1, 3, 24, 16]) /* ty=Tensor[(384, 1, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 1, 3, 16, 384), int8] */;
  %7 = reshape(%6, newshape=[24, 1, 3, 16, 24, 16]) /* ty=Tensor[(24, 1, 3, 16, 24, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 1, 3, 16, 24, 4, 4]) /* ty=Tensor[(24, 1, 3, 16, 24, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 24, 1, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 384, 1, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 8, 6), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 24, 8, 8, 16), 'uint8'), ('TENSOR', (24, 24, 1, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.51122 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.8103 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.81062 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13357 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.10068 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.5428 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.28316 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.42851 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.5806 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.76223 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1168 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07486 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.3381 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.82134 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.48746 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.0867 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.75805 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10822 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07589 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.4204 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.25484 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.51116 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.8299 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.80335 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.46003 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.12151 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.1917 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.14632 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.49454 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.9051 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.82543 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.16395 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09222 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.4658 us/iter
[00:53:20] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.40262 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 384, 8, 8), 'uint8'), ('TENSOR', (384, 384, 1, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:20] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 384, 8, 8), int8], %w: Tensor[(384, 384, 1, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 8, 6), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 384, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(384, 1, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[384, 1, 3, 24, 16]) /* ty=Tensor[(384, 1, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 1, 3, 16, 384), int8] */;
  %7 = reshape(%6, newshape=[24, 1, 3, 16, 24, 16]) /* ty=Tensor[(24, 1, 3, 16, 24, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 1, 3, 16, 24, 4, 4]) /* ty=Tensor[(24, 1, 3, 16, 24, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 24, 1, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 384, 1, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 8, 6), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 24, 8, 8, 16), 'uint8'), ('TENSOR', (24, 24, 1, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.45305 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.4695 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.72457 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09173 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05568 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.8071 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.54235 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.47924 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.4925 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.77395 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.03265 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 1.99608 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.6131 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.49368 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.43474 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.2796 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.85051 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08271 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08837 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.8673 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.37269 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.46168 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.7582 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.71295 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.03638 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.00805 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 14.8835 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.81419 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.49154 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.8957 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.70557 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08457 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04034 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.051 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.23017 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 384, 8, 8), 'uint8'), ('TENSOR', (384, 384, 1, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:21] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 384, 8, 8), int8], %w: Tensor[(384, 384, 1, 3), int8], %b: Tensor[(1, 384, 1, 1), int32]) -> Tensor[(1, 384, 8, 6), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 384, 8, 8), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 384, 8, 8), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 8, 8, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(384, 1, 3, 384), int8] */;
  %5 = reshape(%4, newshape=[384, 1, 3, 24, 16]) /* ty=Tensor[(384, 1, 3, 24, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(24, 1, 3, 16, 384), int8] */;
  %7 = reshape(%6, newshape=[24, 1, 3, 16, 24, 16]) /* ty=Tensor[(24, 1, 3, 16, 24, 16), int8] */;
  %8 = reshape(%7, newshape=[24, 1, 3, 16, 24, 4, 4]) /* ty=Tensor[(24, 1, 3, 16, 24, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(24, 24, 1, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(384, 384, 1, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(384), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(384, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(384, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 384, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 24, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 24, 8, 6, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 384, 8, 6), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 24, 8, 8, 16), 'uint8'), ('TENSOR', (24, 24, 1, 3, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.51795 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.1369 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.75079 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13945 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08492 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.3376 us/iter
[00:53:21] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.44008 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.52532 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.5149 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.71946 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.49978 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09916 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.3305 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.41638 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.56191 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.3782 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.80911 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1688 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.14669 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.3919 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 4.17269 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.60229 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 11.061 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.70997 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14003 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.12157 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.6292 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 5.83123 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 2.59246 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.9458 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.73695 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.17 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.10659 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 15.3586 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 3.80022 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 23, 23), 'uint8'), ('TENSOR', (192, 192, 7, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:22] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 23, 23), int8], %w: Tensor[(192, 192, 7, 1), int8], %b: Tensor[(1, 192, 1, 1), int32]) -> Tensor[(1, 192, 17, 23), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 23, 23), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 23, 23, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 7, 1, 192), int8] */;
  %5 = reshape(%4, newshape=[192, 7, 1, 12, 16]) /* ty=Tensor[(192, 7, 1, 12, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(12, 7, 1, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[12, 7, 1, 16, 12, 16]) /* ty=Tensor[(12, 7, 1, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[12, 7, 1, 16, 12, 4, 4]) /* ty=Tensor[(12, 7, 1, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(12, 12, 7, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=192, kernel_size=[7, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(192, 192, 7, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(192), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(192, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(192, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 192, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 192, 17, 23), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 23, 23, 16), 'uint8'), ('TENSOR', (12, 12, 7, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.58713 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.29744 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.94969 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09225 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05213 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 37.4231 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.86444 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.55669 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.28244 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.91319 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09656 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07531 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 40.2362 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.257 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.67856 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.74731 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.92087 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13756 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.092 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 39.8032 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.4201 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.46481 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.43281 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.92638 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14238 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08906 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.2289 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.6109 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.501 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.68438 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.96081 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13163 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07306 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.8604 us/iter
[00:53:22] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.6576 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 23, 23), 'uint8'), ('TENSOR', (192, 192, 7, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:23] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 23, 23), int8], %w: Tensor[(192, 192, 7, 1), int8], %b: Tensor[(1, 192, 1, 1), int32]) -> Tensor[(1, 192, 17, 23), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 23, 23), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 23, 23, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 7, 1, 192), int8] */;
  %5 = reshape(%4, newshape=[192, 7, 1, 12, 16]) /* ty=Tensor[(192, 7, 1, 12, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(12, 7, 1, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[12, 7, 1, 16, 12, 16]) /* ty=Tensor[(12, 7, 1, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[12, 7, 1, 16, 12, 4, 4]) /* ty=Tensor[(12, 7, 1, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(12, 12, 7, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=192, kernel_size=[7, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(192, 192, 7, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(192), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(192, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(192, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 192, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 192, 17, 23), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 23, 23, 16), 'uint8'), ('TENSOR', (12, 12, 7, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.50169 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.40706 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.91 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.07425 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.03312 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.6582 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.2813 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.60581 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.35644 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.95381 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13169 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.11106 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.8421 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.4406 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.76331 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.17094 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.03437 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.17 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08944 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.1146 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.5677 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.78819 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.90756 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.96794 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1425 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09325 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.2266 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.7263 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.74212 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.62869 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.90594 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13838 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.12019 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 45.2897 us/iter
[00:53:23] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.5841 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 23, 23), 'uint8'), ('TENSOR', (192, 192, 7, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:24] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 23, 23), int8], %w: Tensor[(192, 192, 7, 1), int8], %b: Tensor[(1, 192, 1, 1), int32]) -> Tensor[(1, 192, 17, 23), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 23, 23), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 23, 23, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 7, 1, 192), int8] */;
  %5 = reshape(%4, newshape=[192, 7, 1, 12, 16]) /* ty=Tensor[(192, 7, 1, 12, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(12, 7, 1, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[12, 7, 1, 16, 12, 16]) /* ty=Tensor[(12, 7, 1, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[12, 7, 1, 16, 12, 4, 4]) /* ty=Tensor[(12, 7, 1, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(12, 12, 7, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=192, kernel_size=[7, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(192, 192, 7, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(192), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(192, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(192, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 192, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 192, 17, 23), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 23, 23, 16), 'uint8'), ('TENSOR', (12, 12, 7, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.70844 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.23669 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.89581 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11675 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0985 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.1731 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.2059 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.63931 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.27925 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.87825 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14288 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09825 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 44.7804 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.1335 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.72363 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.82769 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.00031 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.21488 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1705 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.7648 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 11.5369 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.776 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.42938 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.94794 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2205 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.13125 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.715 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 11.0113 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.75344 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.68163 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.92131 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11813 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.15519 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 44.9039 us/iter
[00:53:24] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.7389 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 23, 23), 'uint8'), ('TENSOR', (192, 192, 7, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:25] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 23, 23), int8], %w: Tensor[(192, 192, 7, 1), int8], %b: Tensor[(1, 192, 1, 1), int32]) -> Tensor[(1, 192, 17, 23), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 23, 23), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 23, 23, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 7, 1, 192), int8] */;
  %5 = reshape(%4, newshape=[192, 7, 1, 12, 16]) /* ty=Tensor[(192, 7, 1, 12, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(12, 7, 1, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[12, 7, 1, 16, 12, 16]) /* ty=Tensor[(12, 7, 1, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[12, 7, 1, 16, 12, 4, 4]) /* ty=Tensor[(12, 7, 1, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(12, 12, 7, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=192, kernel_size=[7, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(192, 192, 7, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(192), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(192, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(192, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 192, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 192, 17, 23), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 23, 23, 16), 'uint8'), ('TENSOR', (12, 12, 7, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.68744 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.34775 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.88306 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10756 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.10775 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 45.1451 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.029 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.81494 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.28763 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.89944 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14056 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.10387 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 44.0744 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.082 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.83244 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.01906 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.89837 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13981 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.595 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 45.0569 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.1746 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.77169 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.783 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.83819 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.17812 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.13675 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.3398 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.4911 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.81875 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.78006 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.99413 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.21106 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.18531 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.3658 us/iter
[00:53:25] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.5159 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 23, 23), 'uint8'), ('TENSOR', (192, 192, 7, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:26] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 23, 23), int8], %w: Tensor[(192, 192, 7, 1), int8], %b: Tensor[(1, 192, 1, 1), int32]) -> Tensor[(1, 192, 17, 23), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 23, 23), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 23, 23, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 7, 1, 192), int8] */;
  %5 = reshape(%4, newshape=[192, 7, 1, 12, 16]) /* ty=Tensor[(192, 7, 1, 12, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(12, 7, 1, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[12, 7, 1, 16, 12, 16]) /* ty=Tensor[(12, 7, 1, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[12, 7, 1, 16, 12, 4, 4]) /* ty=Tensor[(12, 7, 1, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(12, 12, 7, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=192, kernel_size=[7, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(192, 192, 7, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(192), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(192, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(192, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 192, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 192, 17, 23), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 23, 23, 16), 'uint8'), ('TENSOR', (12, 12, 7, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.60119 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.64006 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12556 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08425 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 44.0667 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.0149 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.69594 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.38506 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.9375 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09587 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.06838 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.7042 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.3029 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.84812 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.84794 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.02031 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.16206 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04869 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.6345 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 11.2823 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.60112 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.95581 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.03325 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08431 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08631 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.2707 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.4108 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.5535 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.57044 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.95888 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09631 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.073 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.0607 us/iter
[00:53:26] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.2779 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 23, 23), 'uint8'), ('TENSOR', (192, 192, 7, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:26] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 23, 23), int8], %w: Tensor[(192, 192, 7, 1), int8], %b: Tensor[(1, 192, 1, 1), int32]) -> Tensor[(1, 192, 17, 23), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 23, 23), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 23, 23, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 7, 1, 192), int8] */;
  %5 = reshape(%4, newshape=[192, 7, 1, 12, 16]) /* ty=Tensor[(192, 7, 1, 12, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(12, 7, 1, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[12, 7, 1, 16, 12, 16]) /* ty=Tensor[(12, 7, 1, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[12, 7, 1, 16, 12, 4, 4]) /* ty=Tensor[(12, 7, 1, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(12, 12, 7, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=192, kernel_size=[7, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(192, 192, 7, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(192), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(192, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(192, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 192, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 192, 17, 23), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 23, 23, 16), 'uint8'), ('TENSOR', (12, 12, 7, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 6.95812 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.87956 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.96731 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.16819 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.18794 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 39.0592 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 11.0273 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.80075 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.34394 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.90388 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0235 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.10613 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.151 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.2084 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.7845 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.04056 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.02606 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.20444 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.11137 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 39.7846 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.7742 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.95844 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.78269 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.91256 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08713 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.10037 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.7658 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.2697 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.76487 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.69169 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.92394 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08819 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.12956 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 40.0338 us/iter
[00:53:27] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.4803 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 23, 23), 'uint8'), ('TENSOR', (192, 192, 7, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:27] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 23, 23), int8], %w: Tensor[(192, 192, 7, 1), int8], %b: Tensor[(1, 192, 1, 1), int32]) -> Tensor[(1, 192, 17, 23), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 23, 23), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 23, 23, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 7, 1, 192), int8] */;
  %5 = reshape(%4, newshape=[192, 7, 1, 12, 16]) /* ty=Tensor[(192, 7, 1, 12, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(12, 7, 1, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[12, 7, 1, 16, 12, 16]) /* ty=Tensor[(12, 7, 1, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[12, 7, 1, 16, 12, 4, 4]) /* ty=Tensor[(12, 7, 1, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(12, 12, 7, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=192, kernel_size=[7, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(192, 192, 7, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(192), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(192, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(192, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 192, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 192, 17, 23), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 23, 23, 16), 'uint8'), ('TENSOR', (12, 12, 7, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.72956 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.35744 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.68456 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1845 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.08231 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 40.3261 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.1336 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.76306 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.321 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.95462 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09238 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04312 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.7759 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.6813 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.72631 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.494 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.00794 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09281 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04612 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.8609 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.7491 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.76825 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.66181 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.977 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.057 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.06394 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 42.8642 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.6373 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.69794 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.45219 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.95037 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08075 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09512 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.0223 us/iter
[00:53:28] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.6186 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 23, 23), 'uint8'), ('TENSOR', (192, 192, 7, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:28] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 23, 23), int8], %w: Tensor[(192, 192, 7, 1), int8], %b: Tensor[(1, 192, 1, 1), int32]) -> Tensor[(1, 192, 17, 23), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 23, 23), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 23, 23, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 7, 1, 192), int8] */;
  %5 = reshape(%4, newshape=[192, 7, 1, 12, 16]) /* ty=Tensor[(192, 7, 1, 12, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(12, 7, 1, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[12, 7, 1, 16, 12, 16]) /* ty=Tensor[(12, 7, 1, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[12, 7, 1, 16, 12, 4, 4]) /* ty=Tensor[(12, 7, 1, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(12, 12, 7, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=192, kernel_size=[7, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(192, 192, 7, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(192), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(192, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(192, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 192, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 192, 17, 23), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 23, 23, 16), 'uint8'), ('TENSOR', (12, 12, 7, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.81269 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.60006 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.92725 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09975 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.02944 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 41.9231 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.0575 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.8055 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.5125 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.95737 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.07562 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04625 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.7764 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.2828 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.87837 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.15956 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.98 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.14875 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07944 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 44.7121 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.4131 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.82169 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.92806 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.0905 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13837 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.126 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.4914 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.6801 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.86919 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.00406 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.96062 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.13013 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.07462 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 44.5605 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.3871 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 23, 23), 'uint8'), ('TENSOR', (192, 192, 7, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:29] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 23, 23), int8], %w: Tensor[(192, 192, 7, 1), int8], %b: Tensor[(1, 192, 1, 1), int32]) -> Tensor[(1, 192, 17, 23), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 23, 23), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 23, 23, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 7, 1, 192), int8] */;
  %5 = reshape(%4, newshape=[192, 7, 1, 12, 16]) /* ty=Tensor[(192, 7, 1, 12, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(12, 7, 1, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[12, 7, 1, 16, 12, 16]) /* ty=Tensor[(12, 7, 1, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[12, 7, 1, 16, 12, 4, 4]) /* ty=Tensor[(12, 7, 1, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(12, 12, 7, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=192, kernel_size=[7, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(192, 192, 7, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(192), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(192, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(192, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 192, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 192, 17, 23), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 23, 23, 16), 'uint8'), ('TENSOR', (12, 12, 7, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.7445 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.52869 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.92294 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09219 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.01262 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 36.4814 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.97331 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.816 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.58462 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.98731 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10838 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.13381 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 39.2373 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.208 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.60975 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.22531 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.99638 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11825 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.06756 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 38.8846 us/iter
[00:53:29] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.2867 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.73619 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.02606 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.00625 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.11544 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.09575 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 40.1354 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.5371 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.77031 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.64369 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.01731 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.12694 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.06506 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 40.9783 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.4616 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 192, 23, 23), 'uint8'), ('TENSOR', (192, 192, 7, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:30] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 192, 23, 23), int8], %w: Tensor[(192, 192, 7, 1), int8], %b: Tensor[(1, 192, 1, 1), int32]) -> Tensor[(1, 192, 17, 23), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 192, 23, 23), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 192, 23, 23), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 23, 23, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(192, 7, 1, 192), int8] */;
  %5 = reshape(%4, newshape=[192, 7, 1, 12, 16]) /* ty=Tensor[(192, 7, 1, 12, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(12, 7, 1, 16, 192), int8] */;
  %7 = reshape(%6, newshape=[12, 7, 1, 16, 12, 16]) /* ty=Tensor[(12, 7, 1, 16, 12, 16), int8] */;
  %8 = reshape(%7, newshape=[12, 7, 1, 16, 12, 4, 4]) /* ty=Tensor[(12, 7, 1, 16, 12, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(12, 12, 7, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, padding=[0, 0, 0, 0], channels=192, kernel_size=[7, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(192, 192, 7, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(192), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(192, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(192, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 192, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 12, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 12, 17, 23, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 192, 17, 23), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 12, 23, 23, 16), 'uint8'), ('TENSOR', (12, 12, 7, 1, 4, 16, 4), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.56019 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.27587 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.90644 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.09319 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.04075 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 43.7666 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.4763 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.63275 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.34275 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.88575 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.08756 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.06681 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 45.3792 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.6377 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.94613 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.72162 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.95787 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.15006 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1005 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 45.8376 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.8599 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.76631 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.36994 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.94331 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.186 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05113 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.6225 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.5111 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 5.65488 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.74113 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.94469 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.10944 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.05463 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 45.1596 us/iter
[00:53:30] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.6799 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 224, 224), 'uint8'), ('TENSOR', (64, 4, 7, 7), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:31] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 3, 224, 224), int8], %w: Tensor[(64, 3, 7, 7), int8], %b: Tensor[(1, 64, 1, 1), int32]) -> Tensor[(1, 64, 109, 109), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 3, 224, 224), uint8] */;
  %3 = nn.pad(%2, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(1, 4, 224, 224), uint8] */;
  %4 = layout_transform(%3, src_layout="NCHW", dst_layout="NCHW4c") /* ty=Tensor[(1, 1, 224, 224, 4), uint8] */;
  %5 = nn.pad(%w, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(64, 4, 7, 7), int8] */;
  %6 = transpose(%5, axes=[1, 2, 3, 0]) /* ty=Tensor[(4, 7, 7, 64), int8] */;
  %7 = reshape(%6, newshape=[4, 7, 7, 4, 16]) /* ty=Tensor[(4, 7, 7, 4, 16), int8] */;
  %8 = transpose(%7, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(4, 7, 7, 16, 4), int8] */;
  %9 = reshape(%8, newshape=[4, 7, 7, 16, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 4), int8] */;
  %10 = reshape(%9, newshape=[4, 7, 7, 16, 1, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 1, 4), int8] */;
  %11 = transpose(%10, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(4, 1, 7, 7, 1, 16, 4), int8] */;
  %12 = nn.contrib_conv2d_NCHWc(%4, %11, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[7, 7], data_layout="NCHW4c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %13 = cast(%w, dtype="int32") /* ty=Tensor[(64, 3, 7, 7), int32] */;
  %14 = sum(%13, axis=[1, 2, 3]) /* ty=Tensor[(64), int32] */;
  %15 = expand_dims(%14, axis=1, num_newaxis=2) /* ty=Tensor[(64, 1, 1), int32] */;
  %16 = multiply(%15, 128 /* ty=int32 */) /* ty=Tensor[(64, 1, 1), int32] */;
  %17 = expand_dims(%16, axis=0) /* ty=Tensor[(1, 64, 1, 1), int32] */;
  %18 = layout_transform(%17, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %19 = subtract(%12, %18) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %20 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %21 = add(%19, %20) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %22 = multiply(%21, 11 /* ty=int32 */) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  layout_transform(%22, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 64, 109, 109), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1, 224, 224, 4), 'uint8'), ('TENSOR', (4, 1, 7, 7, 1, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW4c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 7.4874 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7862 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1611 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.212 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 230.667 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.0601 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 7.4285 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7385 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3745 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1785 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1677 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 223.139 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 77.6613 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 7.5296 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7069 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4056 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1479 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2437 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 221.922 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.4673 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 7.5064 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.788 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3812 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1514 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 231.056 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.4135 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 7.5905 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.72 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.352 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1744 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2136 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 239.498 us/iter
[00:53:31] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.3501 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 224, 224), 'uint8'), ('TENSOR', (64, 4, 7, 7), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:32] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 3, 224, 224), int8], %w: Tensor[(64, 3, 7, 7), int8], %b: Tensor[(1, 64, 1, 1), int32]) -> Tensor[(1, 64, 109, 109), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 3, 224, 224), uint8] */;
  %3 = nn.pad(%2, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(1, 4, 224, 224), uint8] */;
  %4 = layout_transform(%3, src_layout="NCHW", dst_layout="NCHW4c") /* ty=Tensor[(1, 1, 224, 224, 4), uint8] */;
  %5 = nn.pad(%w, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(64, 4, 7, 7), int8] */;
  %6 = transpose(%5, axes=[1, 2, 3, 0]) /* ty=Tensor[(4, 7, 7, 64), int8] */;
  %7 = reshape(%6, newshape=[4, 7, 7, 4, 16]) /* ty=Tensor[(4, 7, 7, 4, 16), int8] */;
  %8 = transpose(%7, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(4, 7, 7, 16, 4), int8] */;
  %9 = reshape(%8, newshape=[4, 7, 7, 16, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 4), int8] */;
  %10 = reshape(%9, newshape=[4, 7, 7, 16, 1, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 1, 4), int8] */;
  %11 = transpose(%10, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(4, 1, 7, 7, 1, 16, 4), int8] */;
  %12 = nn.contrib_conv2d_NCHWc(%4, %11, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[7, 7], data_layout="NCHW4c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %13 = cast(%w, dtype="int32") /* ty=Tensor[(64, 3, 7, 7), int32] */;
  %14 = sum(%13, axis=[1, 2, 3]) /* ty=Tensor[(64), int32] */;
  %15 = expand_dims(%14, axis=1, num_newaxis=2) /* ty=Tensor[(64, 1, 1), int32] */;
  %16 = multiply(%15, 128 /* ty=int32 */) /* ty=Tensor[(64, 1, 1), int32] */;
  %17 = expand_dims(%16, axis=0) /* ty=Tensor[(1, 64, 1, 1), int32] */;
  %18 = layout_transform(%17, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %19 = subtract(%12, %18) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %20 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %21 = add(%19, %20) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %22 = multiply(%21, 11 /* ty=int32 */) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  layout_transform(%22, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 64, 109, 109), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1, 224, 224, 4), 'uint8'), ('TENSOR', (4, 1, 7, 7, 1, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW4c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.0265 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.6715 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 3.1563 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0948 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0344 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 214.374 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 76.8666 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.0646 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.8009 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3473 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2794 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1623 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 222.818 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 77.2576 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.2557 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7418 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4697 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.137 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1314 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 223.343 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.1376 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.3087 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7437 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.498 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2463 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2937 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 228.286 us/iter
[00:53:32] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 81.1402 us/iter
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.1861 us/iter
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7172 us/iter
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.372 us/iter
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0949 us/iter
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.201 us/iter
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 228.554 us/iter
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.2533 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 224, 224), 'uint8'), ('TENSOR', (64, 4, 7, 7), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:33] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 3, 224, 224), int8], %w: Tensor[(64, 3, 7, 7), int8], %b: Tensor[(1, 64, 1, 1), int32]) -> Tensor[(1, 64, 109, 109), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 3, 224, 224), uint8] */;
  %3 = nn.pad(%2, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(1, 4, 224, 224), uint8] */;
  %4 = layout_transform(%3, src_layout="NCHW", dst_layout="NCHW4c") /* ty=Tensor[(1, 1, 224, 224, 4), uint8] */;
  %5 = nn.pad(%w, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(64, 4, 7, 7), int8] */;
  %6 = transpose(%5, axes=[1, 2, 3, 0]) /* ty=Tensor[(4, 7, 7, 64), int8] */;
  %7 = reshape(%6, newshape=[4, 7, 7, 4, 16]) /* ty=Tensor[(4, 7, 7, 4, 16), int8] */;
  %8 = transpose(%7, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(4, 7, 7, 16, 4), int8] */;
  %9 = reshape(%8, newshape=[4, 7, 7, 16, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 4), int8] */;
  %10 = reshape(%9, newshape=[4, 7, 7, 16, 1, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 1, 4), int8] */;
  %11 = transpose(%10, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(4, 1, 7, 7, 1, 16, 4), int8] */;
  %12 = nn.contrib_conv2d_NCHWc(%4, %11, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[7, 7], data_layout="NCHW4c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %13 = cast(%w, dtype="int32") /* ty=Tensor[(64, 3, 7, 7), int32] */;
  %14 = sum(%13, axis=[1, 2, 3]) /* ty=Tensor[(64), int32] */;
  %15 = expand_dims(%14, axis=1, num_newaxis=2) /* ty=Tensor[(64, 1, 1), int32] */;
  %16 = multiply(%15, 128 /* ty=int32 */) /* ty=Tensor[(64, 1, 1), int32] */;
  %17 = expand_dims(%16, axis=0) /* ty=Tensor[(1, 64, 1, 1), int32] */;
  %18 = layout_transform(%17, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %19 = subtract(%12, %18) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %20 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %21 = add(%19, %20) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %22 = multiply(%21, 11 /* ty=int32 */) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  layout_transform(%22, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 64, 109, 109), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1, 224, 224, 4), 'uint8'), ('TENSOR', (4, 1, 7, 7, 1, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW4c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 7.4373 us/iter
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.6515 us/iter
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 3.0407 us/iter
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1049 us/iter
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0894 us/iter
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 155.256 us/iter
[00:53:33] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 75.9042 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 7.523 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.6915 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4294 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1047 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0615 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 154.18 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.5608 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 7.5522 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7412 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3982 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1587 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1723 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 145.963 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 77.2741 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 7.5143 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7124 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4664 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1772 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1315 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 147.744 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 76.3292 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 9.0966 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.875 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.474 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2175 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1924 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 148.028 us/iter
[00:53:34] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 77.5291 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 224, 224), 'uint8'), ('TENSOR', (64, 4, 7, 7), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:34] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 3, 224, 224), int8], %w: Tensor[(64, 3, 7, 7), int8], %b: Tensor[(1, 64, 1, 1), int32]) -> Tensor[(1, 64, 109, 109), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 3, 224, 224), uint8] */;
  %3 = nn.pad(%2, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(1, 4, 224, 224), uint8] */;
  %4 = layout_transform(%3, src_layout="NCHW", dst_layout="NCHW4c") /* ty=Tensor[(1, 1, 224, 224, 4), uint8] */;
  %5 = nn.pad(%w, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(64, 4, 7, 7), int8] */;
  %6 = transpose(%5, axes=[1, 2, 3, 0]) /* ty=Tensor[(4, 7, 7, 64), int8] */;
  %7 = reshape(%6, newshape=[4, 7, 7, 4, 16]) /* ty=Tensor[(4, 7, 7, 4, 16), int8] */;
  %8 = transpose(%7, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(4, 7, 7, 16, 4), int8] */;
  %9 = reshape(%8, newshape=[4, 7, 7, 16, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 4), int8] */;
  %10 = reshape(%9, newshape=[4, 7, 7, 16, 1, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 1, 4), int8] */;
  %11 = transpose(%10, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(4, 1, 7, 7, 1, 16, 4), int8] */;
  %12 = nn.contrib_conv2d_NCHWc(%4, %11, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[7, 7], data_layout="NCHW4c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %13 = cast(%w, dtype="int32") /* ty=Tensor[(64, 3, 7, 7), int32] */;
  %14 = sum(%13, axis=[1, 2, 3]) /* ty=Tensor[(64), int32] */;
  %15 = expand_dims(%14, axis=1, num_newaxis=2) /* ty=Tensor[(64, 1, 1), int32] */;
  %16 = multiply(%15, 128 /* ty=int32 */) /* ty=Tensor[(64, 1, 1), int32] */;
  %17 = expand_dims(%16, axis=0) /* ty=Tensor[(1, 64, 1, 1), int32] */;
  %18 = layout_transform(%17, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %19 = subtract(%12, %18) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %20 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %21 = add(%19, %20) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %22 = multiply(%21, 11 /* ty=int32 */) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  layout_transform(%22, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 64, 109, 109), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1, 224, 224, 4), 'uint8'), ('TENSOR', (4, 1, 7, 7, 1, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW4c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.4141 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7373 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 3.1537 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2414 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2269 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 206.773 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 80.3303 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.1373 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.6139 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4639 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2062 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2263 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 213.471 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.0513 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.3481 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7787 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.411 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1446 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1917 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 208.722 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 79.7396 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.4033 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7071 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4735 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1993 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2003 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 221.81 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.9819 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.3387 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7006 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4198 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2011 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2614 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 221.655 us/iter
[00:53:35] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.6818 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 224, 224), 'uint8'), ('TENSOR', (64, 4, 7, 7), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:35] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 3, 224, 224), int8], %w: Tensor[(64, 3, 7, 7), int8], %b: Tensor[(1, 64, 1, 1), int32]) -> Tensor[(1, 64, 109, 109), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 3, 224, 224), uint8] */;
  %3 = nn.pad(%2, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(1, 4, 224, 224), uint8] */;
  %4 = layout_transform(%3, src_layout="NCHW", dst_layout="NCHW4c") /* ty=Tensor[(1, 1, 224, 224, 4), uint8] */;
  %5 = nn.pad(%w, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(64, 4, 7, 7), int8] */;
  %6 = transpose(%5, axes=[1, 2, 3, 0]) /* ty=Tensor[(4, 7, 7, 64), int8] */;
  %7 = reshape(%6, newshape=[4, 7, 7, 4, 16]) /* ty=Tensor[(4, 7, 7, 4, 16), int8] */;
  %8 = transpose(%7, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(4, 7, 7, 16, 4), int8] */;
  %9 = reshape(%8, newshape=[4, 7, 7, 16, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 4), int8] */;
  %10 = reshape(%9, newshape=[4, 7, 7, 16, 1, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 1, 4), int8] */;
  %11 = transpose(%10, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(4, 1, 7, 7, 1, 16, 4), int8] */;
  %12 = nn.contrib_conv2d_NCHWc(%4, %11, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[7, 7], data_layout="NCHW4c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %13 = cast(%w, dtype="int32") /* ty=Tensor[(64, 3, 7, 7), int32] */;
  %14 = sum(%13, axis=[1, 2, 3]) /* ty=Tensor[(64), int32] */;
  %15 = expand_dims(%14, axis=1, num_newaxis=2) /* ty=Tensor[(64, 1, 1), int32] */;
  %16 = multiply(%15, 128 /* ty=int32 */) /* ty=Tensor[(64, 1, 1), int32] */;
  %17 = expand_dims(%16, axis=0) /* ty=Tensor[(1, 64, 1, 1), int32] */;
  %18 = layout_transform(%17, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %19 = subtract(%12, %18) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %20 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %21 = add(%19, %20) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %22 = multiply(%21, 11 /* ty=int32 */) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  layout_transform(%22, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 64, 109, 109), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1, 224, 224, 4), 'uint8'), ('TENSOR', (4, 1, 7, 7, 1, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW4c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.491 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7575 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3274 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2269 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0862 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 194.584 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 76.7731 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.5687 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.779 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.354 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1531 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0666 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 198.615 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 77.3719 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.5949 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7125 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 3.1747 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0973 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0793 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 198.767 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 77.8785 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.6713 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.6829 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3709 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1077 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1681 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 192.359 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 79.6936 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.5428 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.729 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.305 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1727 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0813 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 194.273 us/iter
[00:53:36] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 77.0427 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 224, 224), 'uint8'), ('TENSOR', (64, 4, 7, 7), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:36] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 3, 224, 224), int8], %w: Tensor[(64, 3, 7, 7), int8], %b: Tensor[(1, 64, 1, 1), int32]) -> Tensor[(1, 64, 109, 109), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 3, 224, 224), uint8] */;
  %3 = nn.pad(%2, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(1, 4, 224, 224), uint8] */;
  %4 = layout_transform(%3, src_layout="NCHW", dst_layout="NCHW4c") /* ty=Tensor[(1, 1, 224, 224, 4), uint8] */;
  %5 = nn.pad(%w, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(64, 4, 7, 7), int8] */;
  %6 = transpose(%5, axes=[1, 2, 3, 0]) /* ty=Tensor[(4, 7, 7, 64), int8] */;
  %7 = reshape(%6, newshape=[4, 7, 7, 4, 16]) /* ty=Tensor[(4, 7, 7, 4, 16), int8] */;
  %8 = transpose(%7, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(4, 7, 7, 16, 4), int8] */;
  %9 = reshape(%8, newshape=[4, 7, 7, 16, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 4), int8] */;
  %10 = reshape(%9, newshape=[4, 7, 7, 16, 1, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 1, 4), int8] */;
  %11 = transpose(%10, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(4, 1, 7, 7, 1, 16, 4), int8] */;
  %12 = nn.contrib_conv2d_NCHWc(%4, %11, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[7, 7], data_layout="NCHW4c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %13 = cast(%w, dtype="int32") /* ty=Tensor[(64, 3, 7, 7), int32] */;
  %14 = sum(%13, axis=[1, 2, 3]) /* ty=Tensor[(64), int32] */;
  %15 = expand_dims(%14, axis=1, num_newaxis=2) /* ty=Tensor[(64, 1, 1), int32] */;
  %16 = multiply(%15, 128 /* ty=int32 */) /* ty=Tensor[(64, 1, 1), int32] */;
  %17 = expand_dims(%16, axis=0) /* ty=Tensor[(1, 64, 1, 1), int32] */;
  %18 = layout_transform(%17, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %19 = subtract(%12, %18) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %20 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %21 = add(%19, %20) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %22 = multiply(%21, 11 /* ty=int32 */) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  layout_transform(%22, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 64, 109, 109), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1, 224, 224, 4), 'uint8'), ('TENSOR', (4, 1, 7, 7, 1, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW4c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.2491 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.6925 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7086 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1872 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1924 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 171.986 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 79.5492 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.234 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7029 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7633 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1755 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.181 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 169.161 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 79.6916 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.1602 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.78 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.6605 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1556 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1585 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 174.956 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 82.5132 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.575 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.8234 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7636 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3218 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2569 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 176.274 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 80.9378 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.4074 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7238 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.747 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1594 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1167 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 184.015 us/iter
[00:53:37] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 80.4775 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 224, 224), 'uint8'), ('TENSOR', (64, 4, 7, 7), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:37] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 3, 224, 224), int8], %w: Tensor[(64, 3, 7, 7), int8], %b: Tensor[(1, 64, 1, 1), int32]) -> Tensor[(1, 64, 109, 109), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 3, 224, 224), uint8] */;
  %3 = nn.pad(%2, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(1, 4, 224, 224), uint8] */;
  %4 = layout_transform(%3, src_layout="NCHW", dst_layout="NCHW4c") /* ty=Tensor[(1, 1, 224, 224, 4), uint8] */;
  %5 = nn.pad(%w, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(64, 4, 7, 7), int8] */;
  %6 = transpose(%5, axes=[1, 2, 3, 0]) /* ty=Tensor[(4, 7, 7, 64), int8] */;
  %7 = reshape(%6, newshape=[4, 7, 7, 4, 16]) /* ty=Tensor[(4, 7, 7, 4, 16), int8] */;
  %8 = transpose(%7, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(4, 7, 7, 16, 4), int8] */;
  %9 = reshape(%8, newshape=[4, 7, 7, 16, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 4), int8] */;
  %10 = reshape(%9, newshape=[4, 7, 7, 16, 1, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 1, 4), int8] */;
  %11 = transpose(%10, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(4, 1, 7, 7, 1, 16, 4), int8] */;
  %12 = nn.contrib_conv2d_NCHWc(%4, %11, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[7, 7], data_layout="NCHW4c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %13 = cast(%w, dtype="int32") /* ty=Tensor[(64, 3, 7, 7), int32] */;
  %14 = sum(%13, axis=[1, 2, 3]) /* ty=Tensor[(64), int32] */;
  %15 = expand_dims(%14, axis=1, num_newaxis=2) /* ty=Tensor[(64, 1, 1), int32] */;
  %16 = multiply(%15, 128 /* ty=int32 */) /* ty=Tensor[(64, 1, 1), int32] */;
  %17 = expand_dims(%16, axis=0) /* ty=Tensor[(1, 64, 1, 1), int32] */;
  %18 = layout_transform(%17, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %19 = subtract(%12, %18) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %20 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %21 = add(%19, %20) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %22 = multiply(%21, 11 /* ty=int32 */) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  layout_transform(%22, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 64, 109, 109), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1, 224, 224, 4), 'uint8'), ('TENSOR', (4, 1, 7, 7, 1, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW4c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 7.2314 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 3.472 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4133 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1308 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1395 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 164.065 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.8587 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.3657 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7928 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4789 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1702 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2383 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 165.127 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 82.689 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 7.4064 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.8244 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4049 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1116 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1758 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 166.411 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 79.9854 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 7.344 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7344 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3583 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1457 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 3.6202 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 162.516 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 81.8491 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 7.2553 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.665 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3582 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.092 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2095 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 167.885 us/iter
[00:53:38] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 79.7856 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 224, 224), 'uint8'), ('TENSOR', (64, 4, 7, 7), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:38] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 3, 224, 224), int8], %w: Tensor[(64, 3, 7, 7), int8], %b: Tensor[(1, 64, 1, 1), int32]) -> Tensor[(1, 64, 109, 109), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 3, 224, 224), uint8] */;
  %3 = nn.pad(%2, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(1, 4, 224, 224), uint8] */;
  %4 = layout_transform(%3, src_layout="NCHW", dst_layout="NCHW4c") /* ty=Tensor[(1, 1, 224, 224, 4), uint8] */;
  %5 = nn.pad(%w, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(64, 4, 7, 7), int8] */;
  %6 = transpose(%5, axes=[1, 2, 3, 0]) /* ty=Tensor[(4, 7, 7, 64), int8] */;
  %7 = reshape(%6, newshape=[4, 7, 7, 4, 16]) /* ty=Tensor[(4, 7, 7, 4, 16), int8] */;
  %8 = transpose(%7, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(4, 7, 7, 16, 4), int8] */;
  %9 = reshape(%8, newshape=[4, 7, 7, 16, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 4), int8] */;
  %10 = reshape(%9, newshape=[4, 7, 7, 16, 1, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 1, 4), int8] */;
  %11 = transpose(%10, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(4, 1, 7, 7, 1, 16, 4), int8] */;
  %12 = nn.contrib_conv2d_NCHWc(%4, %11, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[7, 7], data_layout="NCHW4c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %13 = cast(%w, dtype="int32") /* ty=Tensor[(64, 3, 7, 7), int32] */;
  %14 = sum(%13, axis=[1, 2, 3]) /* ty=Tensor[(64), int32] */;
  %15 = expand_dims(%14, axis=1, num_newaxis=2) /* ty=Tensor[(64, 1, 1), int32] */;
  %16 = multiply(%15, 128 /* ty=int32 */) /* ty=Tensor[(64, 1, 1), int32] */;
  %17 = expand_dims(%16, axis=0) /* ty=Tensor[(1, 64, 1, 1), int32] */;
  %18 = layout_transform(%17, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %19 = subtract(%12, %18) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %20 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %21 = add(%19, %20) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %22 = multiply(%21, 11 /* ty=int32 */) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  layout_transform(%22, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 64, 109, 109), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1, 224, 224, 4), 'uint8'), ('TENSOR', (4, 1, 7, 7, 1, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW4c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.0411 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7196 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3725 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.121 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.085 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 194.545 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.304 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.3499 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.6715 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4063 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0883 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0835 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 194.55 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.8281 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.321 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.661 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3711 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1744 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1527 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 196.731 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 80.7128 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.2541 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.6585 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4477 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1235 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.112 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 196.433 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 79.6102 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.221 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7075 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.5076 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1557 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1544 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 192.48 us/iter
[00:53:39] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 79.4032 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 224, 224), 'uint8'), ('TENSOR', (64, 4, 7, 7), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:39] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 3, 224, 224), int8], %w: Tensor[(64, 3, 7, 7), int8], %b: Tensor[(1, 64, 1, 1), int32]) -> Tensor[(1, 64, 109, 109), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 3, 224, 224), uint8] */;
  %3 = nn.pad(%2, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(1, 4, 224, 224), uint8] */;
  %4 = layout_transform(%3, src_layout="NCHW", dst_layout="NCHW4c") /* ty=Tensor[(1, 1, 224, 224, 4), uint8] */;
  %5 = nn.pad(%w, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(64, 4, 7, 7), int8] */;
  %6 = transpose(%5, axes=[1, 2, 3, 0]) /* ty=Tensor[(4, 7, 7, 64), int8] */;
  %7 = reshape(%6, newshape=[4, 7, 7, 4, 16]) /* ty=Tensor[(4, 7, 7, 4, 16), int8] */;
  %8 = transpose(%7, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(4, 7, 7, 16, 4), int8] */;
  %9 = reshape(%8, newshape=[4, 7, 7, 16, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 4), int8] */;
  %10 = reshape(%9, newshape=[4, 7, 7, 16, 1, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 1, 4), int8] */;
  %11 = transpose(%10, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(4, 1, 7, 7, 1, 16, 4), int8] */;
  %12 = nn.contrib_conv2d_NCHWc(%4, %11, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[7, 7], data_layout="NCHW4c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %13 = cast(%w, dtype="int32") /* ty=Tensor[(64, 3, 7, 7), int32] */;
  %14 = sum(%13, axis=[1, 2, 3]) /* ty=Tensor[(64), int32] */;
  %15 = expand_dims(%14, axis=1, num_newaxis=2) /* ty=Tensor[(64, 1, 1), int32] */;
  %16 = multiply(%15, 128 /* ty=int32 */) /* ty=Tensor[(64, 1, 1), int32] */;
  %17 = expand_dims(%16, axis=0) /* ty=Tensor[(1, 64, 1, 1), int32] */;
  %18 = layout_transform(%17, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %19 = subtract(%12, %18) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %20 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %21 = add(%19, %20) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %22 = multiply(%21, 11 /* ty=int32 */) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  layout_transform(%22, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 64, 109, 109), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1, 224, 224, 4), 'uint8'), ('TENSOR', (4, 1, 7, 7, 1, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW4c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.1769 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.673 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.2838 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.109 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.164 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 201.426 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 77.9379 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.4516 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.6853 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.4022 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1562 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1569 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 211.314 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.8412 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.2967 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.6009 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3884 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1396 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1466 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 212.511 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.8198 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.5583 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7289 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.399 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1921 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2459 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 216.914 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 79.2043 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.5176 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7345 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.3852 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1397 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2032 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 202.909 us/iter
[00:53:40] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 79.3363 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 4, 224, 224), 'uint8'), ('TENSOR', (64, 4, 7, 7), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:41] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 3, 224, 224), int8], %w: Tensor[(64, 3, 7, 7), int8], %b: Tensor[(1, 64, 1, 1), int32]) -> Tensor[(1, 64, 109, 109), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 3, 224, 224), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 3, 224, 224), uint8] */;
  %3 = nn.pad(%2, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(1, 4, 224, 224), uint8] */;
  %4 = layout_transform(%3, src_layout="NCHW", dst_layout="NCHW4c") /* ty=Tensor[(1, 1, 224, 224, 4), uint8] */;
  %5 = nn.pad(%w, pad_width=[[0, 0], [0, 1], [0, 0], [0, 0]]) /* ty=Tensor[(64, 4, 7, 7), int8] */;
  %6 = transpose(%5, axes=[1, 2, 3, 0]) /* ty=Tensor[(4, 7, 7, 64), int8] */;
  %7 = reshape(%6, newshape=[4, 7, 7, 4, 16]) /* ty=Tensor[(4, 7, 7, 4, 16), int8] */;
  %8 = transpose(%7, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(4, 7, 7, 16, 4), int8] */;
  %9 = reshape(%8, newshape=[4, 7, 7, 16, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 4), int8] */;
  %10 = reshape(%9, newshape=[4, 7, 7, 16, 1, 1, 4]) /* ty=Tensor[(4, 7, 7, 16, 1, 1, 4), int8] */;
  %11 = transpose(%10, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(4, 1, 7, 7, 1, 16, 4), int8] */;
  %12 = nn.contrib_conv2d_NCHWc(%4, %11, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[7, 7], data_layout="NCHW4c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %13 = cast(%w, dtype="int32") /* ty=Tensor[(64, 3, 7, 7), int32] */;
  %14 = sum(%13, axis=[1, 2, 3]) /* ty=Tensor[(64), int32] */;
  %15 = expand_dims(%14, axis=1, num_newaxis=2) /* ty=Tensor[(64, 1, 1), int32] */;
  %16 = multiply(%15, 128 /* ty=int32 */) /* ty=Tensor[(64, 1, 1), int32] */;
  %17 = expand_dims(%16, axis=0) /* ty=Tensor[(1, 64, 1, 1), int32] */;
  %18 = layout_transform(%17, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %19 = subtract(%12, %18) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %20 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 4, 1, 1, 16), int32] */;
  %21 = add(%19, %20) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  %22 = multiply(%21, 11 /* ty=int32 */) /* ty=Tensor[(1, 4, 109, 109, 16), int32] */;
  layout_transform(%22, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 64, 109, 109), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1, 224, 224, 4), 'uint8'), ('TENSOR', (4, 1, 7, 7, 1, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW4c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.3577 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 3.6393 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.641 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.0846 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1617 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 180.359 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 80.0103 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.322 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7673 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7947 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1974 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2708 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 192.009 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 76.5822 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.3564 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.7303 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.7465 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1378 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2161 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 190.172 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 77.7849 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.5642 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 3.0301 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8424 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1888 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2812 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 191.964 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 79.2233 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_nn_pad_layout_transform: 8.6562 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_nn_pad_transpose_reshape_transpose_reshape_reshape_transpose: 2.8393 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 2.8705 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1179 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2354 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 189.84 us/iter
[00:53:41] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 78.9232 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 15, 15), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:42] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 15, 15), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 15, 15), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 15, 15, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 15, 15, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.9465 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.0295 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.79025 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2875 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.19081 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.3444 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.50594 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.106 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.6336 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.98137 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3605 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.19406 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.2566 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.59988 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.8601 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.8909 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.22175 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.32744 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.19825 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 49.297 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.74837 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.1122 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.6986 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.2292 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3986 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2162 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 49.5554 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.2382 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.3164 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.3076 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.80981 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.35725 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.22313 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.6612 us/iter
[00:53:42] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.72069 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 15, 15), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:43] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 15, 15), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 15, 15), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 15, 15, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 15, 15, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.74488 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.0273 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.742 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.25681 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.18913 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.8301 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.45375 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.6384 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.2682 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.80344 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.24212 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.16413 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.6159 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.44475 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.88194 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.9563 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.78956 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.37425 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.964 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 48.7587 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.56625 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.1972 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 21.0215 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.1902 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3028 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2025 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 51.6767 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.7179 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.7438 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.8309 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.10975 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.28006 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.19887 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 48.472 us/iter
[00:53:43] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.55625 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 15, 15), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:43] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 15, 15), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 15, 15), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 15, 15, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 15, 15, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.88306 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.9941 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.64981 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.49188 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.277 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.1114 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.51125 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.86587 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.868 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.07875 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.47538 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.29981 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.7689 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.46587 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.88363 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.7899 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.23706 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.45431 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.32469 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 48.8173 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.6365 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.9755 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.3994 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.09925 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.45469 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.34044 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 49.2121 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 10.0227 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.0116 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.1889 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8802 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.5039 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3304 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 51.4766 us/iter
[00:53:44] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.1591 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 15, 15), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:44] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 15, 15), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 15, 15), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 15, 15, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 15, 15, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.1006 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.8343 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.81269 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.24775 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.14538 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.4486 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.32356 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.0814 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.7759 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.82981 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.27269 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.17475 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 48.2644 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.60163 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.2758 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.5811 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.2896 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3224 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2125 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 48.8494 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.5756 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.1876 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 21.0748 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.18244 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.31356 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.25581 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 49.5176 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.461 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0386 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.5534 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.26131 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.308 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.20962 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 49.6619 us/iter
[00:53:45] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.45663 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 15, 15), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:45] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 15, 15), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 15, 15), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 15, 15, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 15, 15, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.003 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.2545 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 8.66919 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.44144 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.23525 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.3893 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.21769 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.96975 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.2083 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.83181 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.37887 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.22494 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.3452 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.34644 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.0453 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.8632 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.13725 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.37337 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.18625 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 48.5988 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.425 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.2767 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.2112 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.80194 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.38131 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.23906 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.6632 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.48275 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.1295 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.5807 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8549 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3781 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2527 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 50.2012 us/iter
[00:53:46] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.6026 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 15, 15), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:46] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 15, 15), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 15, 15), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 15, 15, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 15, 15, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.1157 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.1157 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.88119 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.35994 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.23587 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.6821 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.38975 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.103 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.2339 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.81794 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.37362 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.26675 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.5396 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.45219 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.0021 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.3497 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.27313 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.34112 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.29138 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 49.0576 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.46488 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.93062 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.3221 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.3485 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.35944 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.29319 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 49.6356 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.456 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.1551 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.1574 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.84713 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.33844 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.33281 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 47.3818 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.72188 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 15, 15), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:47] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 15, 15), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 15, 15), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 15, 15, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 15, 15, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.96331 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.4181 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.03106 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.39938 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.27325 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.3994 us/iter
[00:53:47] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.32919 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.134 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.0987 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.98638 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.43387 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.25094 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 47.7385 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.5595 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9172 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 21.2026 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.9504 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4311 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2922 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 48.2429 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.7157 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.3808 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 21.6378 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.2579 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4892 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2739 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 49.6278 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.4657 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.206 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.1419 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 9.038 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4397 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3278 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 50.9427 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.6031 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 15, 15), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:48] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 15, 15), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 15, 15), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 15, 15, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 15, 15, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.2522 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.9259 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.70137 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.40569 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2025 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 48.1154 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.37869 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.99862 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.9418 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.78019 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.37862 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.16019 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.6546 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.46506 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.8886 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.7457 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.166 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 4.0791 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.3427 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 49.7681 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.0482 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 12.4414 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.0062 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.006 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.4491 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2429 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 52.5633 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.5707 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 12.6719 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.6018 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.8286 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3489 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2364 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 48.0091 us/iter
[00:53:48] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.3317 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 15, 15), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:49] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 15, 15), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 15, 15), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 15, 15, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 15, 15, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.79219 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.6157 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.97412 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.27481 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.23381 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.8806 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.37781 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.77631 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.8474 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.71944 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.25012 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1775 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.8822 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.338 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.65156 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.8275 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.12425 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.25125 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.215 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 49.1843 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.31313 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9304 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.2232 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.3999 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2459 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2731 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 51.8844 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 9.4096 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.87063 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 19.7027 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.07969 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.24687 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.21456 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 48.8299 us/iter
[00:53:49] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.50687 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 1024, 15, 15), 'uint8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:50] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 1024, 15, 15), int8], %w: Tensor[(2048, 1024, 1, 1), int8], %b: Tensor[(1, 2048, 1, 1), int32]) -> Tensor[(1, 2048, 8, 8), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 1024, 15, 15), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 1024, 15, 15), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 64, 15, 15, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(1024, 1, 1, 2048), int8] */;
  %5 = reshape(%4, newshape=[1024, 1, 1, 128, 16]) /* ty=Tensor[(1024, 1, 1, 128, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(128, 1, 1, 16, 1024), int8] */;
  %7 = reshape(%6, newshape=[128, 1, 1, 16, 64, 16]) /* ty=Tensor[(128, 1, 1, 16, 64, 16), int8] */;
  %8 = reshape(%7, newshape=[128, 1, 1, 16, 64, 4, 4]) /* ty=Tensor[(128, 1, 1, 16, 64, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(128, 64, 1, 1, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(2048, 1024, 1, 1), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(2048), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(2048, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(2048, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 2048, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 128, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 128, 8, 8, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 2048, 8, 8), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 64, 15, 15, 16), 'uint8'), ('TENSOR', (128, 64, 1, 1, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.04 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.9084 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 14.9264 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.37344 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.19237 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.5697 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.35975 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 9.95612 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.9371 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.77794 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.33419 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.25706 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.593 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.4055 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0978 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 22.7164 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.9621 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3476 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2223 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 48.8172 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.5426 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.1128 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 20.3656 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 7.01006 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.36644 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.31369 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.5513 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.63438 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.2459 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 18.8812 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 6.919 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.33194 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.22187 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 46.9106 us/iter
[00:53:50] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 8.62875 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 128, 65, 65), 'uint8'), ('TENSOR', (256, 128, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:51] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 128, 65, 65), int8], %w: Tensor[(256, 128, 3, 3), int8], %b: Tensor[(1, 256, 1, 1), int32]) -> Tensor[(1, 256, 32, 32), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 128, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(128, 3, 3, 256), int8] */;
  %5 = reshape(%4, newshape=[128, 3, 3, 16, 16]) /* ty=Tensor[(128, 3, 3, 16, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(16, 3, 3, 16, 128), int8] */;
  %7 = reshape(%6, newshape=[16, 3, 3, 16, 8, 16]) /* ty=Tensor[(16, 3, 3, 16, 8, 16), int8] */;
  %8 = reshape(%7, newshape=[16, 3, 3, 16, 8, 4, 4]) /* ty=Tensor[(16, 3, 3, 16, 8, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(16, 8, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(256, 128, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(256), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(256, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(256, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 256, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 256, 32, 32), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 8, 65, 65, 16), 'uint8'), ('TENSOR', (16, 8, 3, 3, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0341 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0258 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5761 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1012 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1155 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 77.0393 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.7985 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.032 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9378 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5775 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.146 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1279 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.1701 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.5674 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0499 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9103 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5732 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1859 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1488 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.4673 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.4771 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0096 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9958 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5339 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1784 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2076 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 80.7771 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.6969 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9742 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9166 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5379 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1731 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1407 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.3211 us/iter
[00:53:51] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.5354 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 128, 65, 65), 'uint8'), ('TENSOR', (256, 128, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:52] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 128, 65, 65), int8], %w: Tensor[(256, 128, 3, 3), int8], %b: Tensor[(1, 256, 1, 1), int32]) -> Tensor[(1, 256, 32, 32), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 128, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(128, 3, 3, 256), int8] */;
  %5 = reshape(%4, newshape=[128, 3, 3, 16, 16]) /* ty=Tensor[(128, 3, 3, 16, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(16, 3, 3, 16, 128), int8] */;
  %7 = reshape(%6, newshape=[16, 3, 3, 16, 8, 16]) /* ty=Tensor[(16, 3, 3, 16, 8, 16), int8] */;
  %8 = reshape(%7, newshape=[16, 3, 3, 16, 8, 4, 4]) /* ty=Tensor[(16, 3, 3, 16, 8, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(16, 8, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(256, 128, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(256), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(256, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(256, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 256, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 256, 32, 32), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 8, 65, 65, 16), 'uint8'), ('TENSOR', (16, 8, 3, 3, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0922 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0111 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5673 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1782 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1947 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.6603 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.8583 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9892 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.878 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5344 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2037 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1584 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.6869 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 27.3395 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0085 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9145 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5829 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1992 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1894 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.7241 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 27.1908 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0186 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9374 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.546 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.165 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2306 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.555 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 27.4456 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1178 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.8892 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5546 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2537 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1784 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.6536 us/iter
[00:53:52] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 27.5868 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 128, 65, 65), 'uint8'), ('TENSOR', (256, 128, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:53] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 128, 65, 65), int8], %w: Tensor[(256, 128, 3, 3), int8], %b: Tensor[(1, 256, 1, 1), int32]) -> Tensor[(1, 256, 32, 32), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 128, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(128, 3, 3, 256), int8] */;
  %5 = reshape(%4, newshape=[128, 3, 3, 16, 16]) /* ty=Tensor[(128, 3, 3, 16, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(16, 3, 3, 16, 128), int8] */;
  %7 = reshape(%6, newshape=[16, 3, 3, 16, 8, 16]) /* ty=Tensor[(16, 3, 3, 16, 8, 16), int8] */;
  %8 = reshape(%7, newshape=[16, 3, 3, 16, 8, 4, 4]) /* ty=Tensor[(16, 3, 3, 16, 8, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(16, 8, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(256, 128, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(256), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(256, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(256, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 256, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 256, 32, 32), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 8, 65, 65, 16), 'uint8'), ('TENSOR', (16, 8, 3, 3, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.006 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0475 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5095 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1214 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0469 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.1312 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.4367 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.9687 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0445 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5992 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1713 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1577 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.6228 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.5284 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1852 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9985 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.4875 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1455 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1231 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.8614 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 27.0362 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.079 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.3154 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5158 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1193 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1323 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 80.7664 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.8142 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9359 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0442 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.4985 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1153 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0666 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 80.6928 us/iter
[00:53:53] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.684 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 128, 65, 65), 'uint8'), ('TENSOR', (256, 128, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:54] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 128, 65, 65), int8], %w: Tensor[(256, 128, 3, 3), int8], %b: Tensor[(1, 256, 1, 1), int32]) -> Tensor[(1, 256, 32, 32), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 128, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(128, 3, 3, 256), int8] */;
  %5 = reshape(%4, newshape=[128, 3, 3, 16, 16]) /* ty=Tensor[(128, 3, 3, 16, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(16, 3, 3, 16, 128), int8] */;
  %7 = reshape(%6, newshape=[16, 3, 3, 16, 8, 16]) /* ty=Tensor[(16, 3, 3, 16, 8, 16), int8] */;
  %8 = reshape(%7, newshape=[16, 3, 3, 16, 8, 4, 4]) /* ty=Tensor[(16, 3, 3, 16, 8, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(16, 8, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(256, 128, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(256), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(256, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(256, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 256, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 256, 32, 32), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 8, 65, 65, 16), 'uint8'), ('TENSOR', (16, 8, 3, 3, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0049 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9727 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5352 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1804 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1579 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.1187 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.0663 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0006 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.8805 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5353 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1973 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1307 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.1028 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 25.9391 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.045 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0081 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5355 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1713 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1271 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.1032 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.7877 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.987 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9001 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5632 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1918 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1375 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.2252 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.3387 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9956 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9089 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5624 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2063 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.144 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.9704 us/iter
[00:53:54] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.2991 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 128, 65, 65), 'uint8'), ('TENSOR', (256, 128, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:55] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 128, 65, 65), int8], %w: Tensor[(256, 128, 3, 3), int8], %b: Tensor[(1, 256, 1, 1), int32]) -> Tensor[(1, 256, 32, 32), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 128, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(128, 3, 3, 256), int8] */;
  %5 = reshape(%4, newshape=[128, 3, 3, 16, 16]) /* ty=Tensor[(128, 3, 3, 16, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(16, 3, 3, 16, 128), int8] */;
  %7 = reshape(%6, newshape=[16, 3, 3, 16, 8, 16]) /* ty=Tensor[(16, 3, 3, 16, 8, 16), int8] */;
  %8 = reshape(%7, newshape=[16, 3, 3, 16, 8, 4, 4]) /* ty=Tensor[(16, 3, 3, 16, 8, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(16, 8, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(256, 128, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(256), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(256, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(256, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 256, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 256, 32, 32), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 8, 65, 65, 16), 'uint8'), ('TENSOR', (16, 8, 3, 3, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1985 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0851 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.6206 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2802 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1706 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.7313 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 27.8413 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0392 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0399 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.6377 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2417 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2318 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 77.5672 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 27.0767 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.9942 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 9.8292 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.864 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3039 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1733 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 77.7939 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.9334 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.028 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.005 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.6404 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2682 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2536 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 77.5026 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.8449 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0191 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9222 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.6565 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.23 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2162 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 77.9427 us/iter
[00:53:55] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.5942 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 128, 65, 65), 'uint8'), ('TENSOR', (256, 128, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:56] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 128, 65, 65), int8], %w: Tensor[(256, 128, 3, 3), int8], %b: Tensor[(1, 256, 1, 1), int32]) -> Tensor[(1, 256, 32, 32), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 128, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(128, 3, 3, 256), int8] */;
  %5 = reshape(%4, newshape=[128, 3, 3, 16, 16]) /* ty=Tensor[(128, 3, 3, 16, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(16, 3, 3, 16, 128), int8] */;
  %7 = reshape(%6, newshape=[16, 3, 3, 16, 8, 16]) /* ty=Tensor[(16, 3, 3, 16, 8, 16), int8] */;
  %8 = reshape(%7, newshape=[16, 3, 3, 16, 8, 4, 4]) /* ty=Tensor[(16, 3, 3, 16, 8, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(16, 8, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(256, 128, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(256), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(256, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(256, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 256, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 256, 32, 32), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 8, 65, 65, 16), 'uint8'), ('TENSOR', (16, 8, 3, 3, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0303 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.097 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5359 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1454 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1534 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 77.966 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.7524 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9717 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.1683 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5992 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1567 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1359 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.0812 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.5031 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1079 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.5489 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.6534 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1819 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1288 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.6504 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.8016 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9993 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 9.1613 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.6445 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.116 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1337 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 80.3275 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.5722 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0223 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.6824 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.6629 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1602 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1343 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.2077 us/iter
[00:53:56] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.5328 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 128, 65, 65), 'uint8'), ('TENSOR', (256, 128, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:57] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 128, 65, 65), int8], %w: Tensor[(256, 128, 3, 3), int8], %b: Tensor[(1, 256, 1, 1), int32]) -> Tensor[(1, 256, 32, 32), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 128, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(128, 3, 3, 256), int8] */;
  %5 = reshape(%4, newshape=[128, 3, 3, 16, 16]) /* ty=Tensor[(128, 3, 3, 16, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(16, 3, 3, 16, 128), int8] */;
  %7 = reshape(%6, newshape=[16, 3, 3, 16, 8, 16]) /* ty=Tensor[(16, 3, 3, 16, 8, 16), int8] */;
  %8 = reshape(%7, newshape=[16, 3, 3, 16, 8, 4, 4]) /* ty=Tensor[(16, 3, 3, 16, 8, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(16, 8, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(256, 128, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(256), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(256, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(256, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 256, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 256, 32, 32), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 8, 65, 65, 16), 'uint8'), ('TENSOR', (16, 8, 3, 3, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0446 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0641 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5122 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1907 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1814 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 73.1432 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 27.0429 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0261 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0272 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.586 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2294 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1723 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 80.5116 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 27.0349 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9877 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0359 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5691 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2099 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1726 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 80.0738 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.968 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0289 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9928 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 5.9158 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2785 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.196 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 77.8232 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.7958 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0144 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0819 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5791 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2113 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1697 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.5079 us/iter
[00:53:57] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.8278 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 128, 65, 65), 'uint8'), ('TENSOR', (256, 128, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:58] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 128, 65, 65), int8], %w: Tensor[(256, 128, 3, 3), int8], %b: Tensor[(1, 256, 1, 1), int32]) -> Tensor[(1, 256, 32, 32), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 128, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(128, 3, 3, 256), int8] */;
  %5 = reshape(%4, newshape=[128, 3, 3, 16, 16]) /* ty=Tensor[(128, 3, 3, 16, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(16, 3, 3, 16, 128), int8] */;
  %7 = reshape(%6, newshape=[16, 3, 3, 16, 8, 16]) /* ty=Tensor[(16, 3, 3, 16, 8, 16), int8] */;
  %8 = reshape(%7, newshape=[16, 3, 3, 16, 8, 4, 4]) /* ty=Tensor[(16, 3, 3, 16, 8, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(16, 8, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(256, 128, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(256), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(256, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(256, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 256, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 256, 32, 32), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 8, 65, 65, 16), 'uint8'), ('TENSOR', (16, 8, 3, 3, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 13.3747 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.399 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.578 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1386 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1357 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.3979 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.8194 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0017 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9028 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5152 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1343 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.064 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.2105 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.5648 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1923 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0384 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5732 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1017 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0862 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.7604 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 28.6672 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0154 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9318 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.6385 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1176 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1424 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 81.266 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.5913 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.2507 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 10.7453 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5529 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1947 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1106 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.7393 us/iter
[00:53:58] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 27.9599 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 128, 65, 65), 'uint8'), ('TENSOR', (256, 128, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:59] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 128, 65, 65), int8], %w: Tensor[(256, 128, 3, 3), int8], %b: Tensor[(1, 256, 1, 1), int32]) -> Tensor[(1, 256, 32, 32), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 128, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(128, 3, 3, 256), int8] */;
  %5 = reshape(%4, newshape=[128, 3, 3, 16, 16]) /* ty=Tensor[(128, 3, 3, 16, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(16, 3, 3, 16, 128), int8] */;
  %7 = reshape(%6, newshape=[16, 3, 3, 16, 8, 16]) /* ty=Tensor[(16, 3, 3, 16, 8, 16), int8] */;
  %8 = reshape(%7, newshape=[16, 3, 3, 16, 8, 4, 4]) /* ty=Tensor[(16, 3, 3, 16, 8, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(16, 8, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(256, 128, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(256), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(256, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(256, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 256, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 256, 32, 32), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 8, 65, 65, 16), 'uint8'), ('TENSOR', (16, 8, 3, 3, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.2122 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.451 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.8602 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.3358 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2527 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 80.5626 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.8397 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1077 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9583 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.614 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2332 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2535 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 80.6866 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.9749 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0658 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0765 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.6048 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2762 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2062 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.8206 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.7598 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.1002 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9538 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.6518 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2898 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2052 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 79.0193 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 27.3166 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 11.0146 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 7.9702 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5487 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1988 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1708 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.865 us/iter
[00:53:59] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 27.1881 us/iter
Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 128, 65, 65), 'uint8'), ('TENSOR', (256, 128, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:53:59] /home/ubuntu/tvm/src/relay/backend/build_module.cc:326: v0.0.4
def @main(%x: Tensor[(1, 128, 65, 65), int8], %w: Tensor[(256, 128, 3, 3), int8], %b: Tensor[(1, 256, 1, 1), int32]) -> Tensor[(1, 256, 32, 32), int32] {
  %0 = cast(%x, dtype="int32") /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %1 = add(%0, 128 /* ty=int32 */) /* ty=Tensor[(1, 128, 65, 65), int32] */;
  %2 = cast(%1, dtype="uint8") /* ty=Tensor[(1, 128, 65, 65), uint8] */;
  %3 = layout_transform(%2, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 8, 65, 65, 16), uint8] */;
  %4 = transpose(%w, axes=[1, 2, 3, 0]) /* ty=Tensor[(128, 3, 3, 256), int8] */;
  %5 = reshape(%4, newshape=[128, 3, 3, 16, 16]) /* ty=Tensor[(128, 3, 3, 16, 16), int8] */;
  %6 = transpose(%5, axes=[3, 1, 2, 4, 0]) /* ty=Tensor[(16, 3, 3, 16, 128), int8] */;
  %7 = reshape(%6, newshape=[16, 3, 3, 16, 8, 16]) /* ty=Tensor[(16, 3, 3, 16, 8, 16), int8] */;
  %8 = reshape(%7, newshape=[16, 3, 3, 16, 8, 4, 4]) /* ty=Tensor[(16, 3, 3, 16, 8, 4, 4), int8] */;
  %9 = transpose(%8, axes=[0, 4, 1, 2, 5, 3, 6]) /* ty=Tensor[(16, 8, 3, 3, 4, 16, 4), int8] */;
  %10 = nn.contrib_conv2d_NCHWc(%3, %9, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[3, 3], data_layout="NCHW16c", out_layout="NCHW16c", out_dtype="int32") /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %11 = cast(%w, dtype="int32") /* ty=Tensor[(256, 128, 3, 3), int32] */;
  %12 = sum(%11, axis=[1, 2, 3]) /* ty=Tensor[(256), int32] */;
  %13 = expand_dims(%12, axis=1, num_newaxis=2) /* ty=Tensor[(256, 1, 1), int32] */;
  %14 = multiply(%13, 128 /* ty=int32 */) /* ty=Tensor[(256, 1, 1), int32] */;
  %15 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 256, 1, 1), int32] */;
  %16 = layout_transform(%15, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %17 = subtract(%10, %16) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %18 = layout_transform(%b, src_layout="NCHW", dst_layout="NCHW16c") /* ty=Tensor[(1, 16, 1, 1, 16), int32] */;
  %19 = add(%17, %18) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  %20 = multiply(%19, 11 /* ty=int32 */) /* ty=Tensor[(1, 16, 32, 32, 16), int32] */;
  layout_transform(%20, src_layout="NCHW16c", dst_layout="NCHW") /* ty=Tensor[(1, 256, 32, 32), int32] */
}

Cannot find config for target=llvm -keys=cpu -mcpu=cascadelake, workload=('conv2d_NCHWc_int8.x86', ('TENSOR', (1, 8, 65, 65, 16), 'uint8'), ('TENSOR', (16, 8, 3, 3, 4, 16, 4), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW16c', 'NCHW16c', 'int32'). A fallback configuration is used, which may bring great performance regression.
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9187 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0422 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5712 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.1955 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1804 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 76.958 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.4108 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9555 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0647 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5594 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2918 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.0804 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.7918 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.5031 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.996 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0538 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5755 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2466 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.2077 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.5185 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.67 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9466 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 8.0528 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.5937 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2509 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.126 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.9198 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.3686 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:95: Iteration: 0
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #0 fused_cast_add_cast_layout_transform: 10.9782 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #1 fused_transpose_reshape_transpose_reshape_reshape_transpose: 9.3161 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #2 fused_cast_sum: 4.6085 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #3 fused_expand_dims_multiply_expand_dims_layout_transform: 2.2142 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #4 fused_layout_transform_1: 2.1818 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #5 fused_nn_contrib_conv2d_NCHWc_subtract_add_multiply: 78.8997 us/iter
[00:54:00] /home/ubuntu/tvm/src/runtime/graph/debug/graph_runtime_debug.cc:100: Op #6 fused_layout_transform: 26.6956 us/iter
!!! (1024, 16, 16, 2048, 1024, 1, 1, 1, 1) 178.59739999999996
!!! (960, 8, 8, 160, 960, 1, 1, 1, 1) 10.4121
!!! (256, 56, 56, 512, 256, 1, 1, 1, 1) 111.23030000000001
!!! (64, 56, 56, 128, 64, 1, 1, 1, 1) 38.61
!!! (512, 10, 10, 512, 512, 3, 3, 1, 1) 41.0131
!!! (192, 16, 16, 160, 192, 3, 3, 1, 1) 24.963999999999995
!!! (64, 65, 65, 128, 64, 3, 3, 1, 1) 132.1628
!!! (320, 65, 65, 384, 320, 3, 3, 1, 1) 1002.1809000000001
!!! (384, 8, 8, 384, 384, 1, 3, 1, 1) 14.962099999999998
!!! (192, 23, 23, 192, 192, 7, 1, 1, 1) 40.687799999999996
!!! (3, 224, 224, 64, 3, 7, 7, 2, 2) 188.7333
!!! (1024, 15, 15, 2048, 1024, 1, 1, 2, 2) 46.48039999999999
!!! (128, 65, 65, 256, 128, 3, 3, 2, 2) 77.6599
